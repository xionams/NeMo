<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="NeMo RNNT Training with Numba FP16 Support"><meta name=author content="['Somshubra Majumdar', 'Graham Markall']"><link href=https://nvidia.github.io/NeMo/blogs/2023/2023-10-28-numba-fp16/ rel=canonical><link href=../2023-08-forced-alignment/ rel=prev><link href=../../2024/2024-01-parakeet/ rel=next><link rel=icon href=../../../assets/favicon.png><meta name=generator content="mkdocs-1.5.3, mkdocs-material-9.5.6"><title>Training NeMo RNN-T Models Efficiently with Numba FP16 Support - NVIDIA NeMo</title><link rel=stylesheet href=../../../assets/stylesheets/main.50c56a3b.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../assets/stylesheets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><!-- Add scripts that need to run before here --><!-- Add scripts that need to run afterward here --><!-- Add Twitter Card metadata --><meta name=twitter:card content=summary_large_image><!-- Add OpenGraph Title metadata --><meta property=og:title content="Training NeMo RNN-T Models Efficiently with Numba FP16 Support"><meta name=twitter:title content="Training NeMo RNN-T Models Efficiently with Numba FP16 Support"><!-- Add OpenGraph Type metadata --><meta property=og:type content=website><!-- Add OpenGraph URL metadata --><meta content=https://nvidia.github.io/NeMo/blogs/2023/2023-10-28-numba-fp16/ property=og:url><!-- Add OpenGraph Image metadata --><meta property=og:image content=https://github.com/NVIDIA/NeMo/releases/download/v1.20.0/asset-post-2023-10-28-numba-fp16-rnnt_joint.png><meta property=twitter:image content=https://github.com/NVIDIA/NeMo/releases/download/v1.20.0/asset-post-2023-10-28-numba-fp16-rnnt_joint.png><!-- Add OpenGraph Image Type metadata --><meta property=og:image:type content=image/png><!-- Add OpenGraph Description metadata --><meta property=og:description content="NeMo RNNT Training with Numba FP16 Support"><meta property=twitter:description content="NeMo RNNT Training with Numba FP16 Support"></head> <body dir=ltr data-md-color-scheme=midnight-black data-md-color-primary=indigo data-md-color-accent=light-blue> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#training-nemo-rnn-t-models-efficiently-with-numba-fp16-support class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="NVIDIA NeMo" class="md-header__button md-logo" aria-label="NVIDIA NeMo" data-md-component=logo> <svg id=Layer_1 data-name="Layer 1" xmlns=http://www.w3.org/2000/svg viewbox="0 0 16 16"> <defs> <style>
      .cls-1 {
        fill: #76b900;
      }
    </style> </defs> <g id=NVIDIA_Logo data-name="NVIDIA Logo"> <path id=Eye_Mark class=cls-1 d=M5.9698,5.86543V4.90907c.09285-.00661.18663-.01156.28219-.01456A5.75152,5.75152,0,0,1,10.58372,7.142S8.73032,9.71631,6.74309,9.71631a2.40975,2.40975,0,0,1-.77329-.12364v-2.9c1.01832.123,1.223.57279,1.83539,1.59325l1.36157-1.148A3.60517,3.60517,0,0,0,6.49742,5.83431a4.93745,4.93745,0,0,0-.52762.03112m0-3.15922V4.13474c.09389-.00742.1879-.0134.28219-.0168,3.63754-.12254,6.0073,2.98317,6.0073,2.98317s-2.722,3.31-5.55774,3.31a4.18488,4.18488,0,0,1-.73175-.06444v.883a4.81728,4.81728,0,0,0,.60938.03947c2.639,0,4.54736-1.34759,6.39542-2.94267.30618.24532,1.56062.8421,1.8186,1.1037-1.75722,1.47088-5.852,2.65644-8.17346,2.65644-.22369,0-.43886-.01352-.64994-.03376v1.241H16V2.70621Zm0,6.88646v.754A4.26109,4.26109,0,0,1,2.85159,7.37428,5.27645,5.27645,0,0,1,5.9698,5.86543v.8272l-.0038-.0004a2.34214,2.34214,0,0,0-1.81935.83163A3.25091,3.25091,0,0,0,5.9698,9.59267M1.63473,7.26433A6.045,6.045,0,0,1,5.9698,4.90907V4.13474C2.77053,4.39151,0,7.10111,0,7.10111s1.56908,4.53638,5.9698,4.95171v-.82318C2.74044,10.82334,1.63473,7.26433,1.63473,7.26433Z data-name="Eye Mark"/> </g> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> NVIDIA NeMo </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Training NeMo RNN-T Models Efficiently with Numba FP16 Support </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=midnight-black data-md-color-primary=indigo data-md-color-accent=light-blue aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=light-blue aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg> </label> </form> <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/NVIDIA/NeMo title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> NVIDIA/NeMo </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Blog </a> </li> <li class=md-tabs__item> <a href=../../../publications/ class=md-tabs__link> Publications </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="NVIDIA NeMo" class="md-nav__button md-logo" aria-label="NVIDIA NeMo" data-md-component=logo> <svg id=Layer_1 data-name="Layer 1" xmlns=http://www.w3.org/2000/svg viewbox="0 0 16 16"> <defs> <style>
      .cls-1 {
        fill: #76b900;
      }
    </style> </defs> <g id=NVIDIA_Logo data-name="NVIDIA Logo"> <path id=Eye_Mark class=cls-1 d=M5.9698,5.86543V4.90907c.09285-.00661.18663-.01156.28219-.01456A5.75152,5.75152,0,0,1,10.58372,7.142S8.73032,9.71631,6.74309,9.71631a2.40975,2.40975,0,0,1-.77329-.12364v-2.9c1.01832.123,1.223.57279,1.83539,1.59325l1.36157-1.148A3.60517,3.60517,0,0,0,6.49742,5.83431a4.93745,4.93745,0,0,0-.52762.03112m0-3.15922V4.13474c.09389-.00742.1879-.0134.28219-.0168,3.63754-.12254,6.0073,2.98317,6.0073,2.98317s-2.722,3.31-5.55774,3.31a4.18488,4.18488,0,0,1-.73175-.06444v.883a4.81728,4.81728,0,0,0,.60938.03947c2.639,0,4.54736-1.34759,6.39542-2.94267.30618.24532,1.56062.8421,1.8186,1.1037-1.75722,1.47088-5.852,2.65644-8.17346,2.65644-.22369,0-.43886-.01352-.64994-.03376v1.241H16V2.70621Zm0,6.88646v.754A4.26109,4.26109,0,0,1,2.85159,7.37428,5.27645,5.27645,0,0,1,5.9698,5.86543v.8272l-.0038-.0004a2.34214,2.34214,0,0,0-1.81935.83163A3.25091,3.25091,0,0,0,5.9698,9.59267M1.63473,7.26433A6.045,6.045,0,0,1,5.9698,4.90907V4.13474C2.77053,4.39151,0,7.10111,0,7.10111s1.56908,4.53638,5.9698,4.95171v-.82318C2.74044,10.82334,1.63473,7.26433,1.63473,7.26433Z data-name="Eye Mark"/> </g> </svg> </a> NVIDIA NeMo </label> <div class=md-nav__source> <a href=https://github.com/NVIDIA/NeMo title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> NVIDIA/NeMo </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Blog </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../archive/2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> <li class=md-nav__item> <a href=../../archive/2023/ class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> <li class=md-nav__item> <a href=../../archive/2022/ class=md-nav__link> <span class=md-ellipsis> 2022 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../category/announcements/ class=md-nav__link> <span class=md-ellipsis> Announcements </span> </a> </li> <li class=md-nav__item> <a href=../../category/nvidia-technical-blog/ class=md-nav__link> <span class=md-ellipsis> NVIDIA Technical blog </span> </a> </li> <li class=md-nav__item> <a href=../../category/papers/ class=md-nav__link> <span class=md-ellipsis> Papers </span> </a> </li> <li class=md-nav__item> <a href=../../category/technical-deep-dive/ class=md-nav__link> <span class=md-ellipsis> Technical deep-dive </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../../../publications/ class="md-nav__link "> <span class=md-ellipsis> Publications </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Publications </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../publications/archive/2023/ class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/archive/2022/ class=md-nav__link> <span class=md-ellipsis> 2022 </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/archive/2021/ class=md-nav__link> <span class=md-ellipsis> 2021 </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/archive/2020/ class=md-nav__link> <span class=md-ellipsis> 2020 </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/archive/2019/ class=md-nav__link> <span class=md-ellipsis> 2019 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_3> <label class=md-nav__link for=__nav_3_3 id=__nav_3_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../publications/category/inverse-text-normalization/ class=md-nav__link> <span class=md-ellipsis> (Inverse) Text Normalization </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/automatic-speech-recognition/ class=md-nav__link> <span class=md-ellipsis> Automatic Speech Recognition </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/dialog-state-tracking/ class=md-nav__link> <span class=md-ellipsis> Dialog State Tracking </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/large-language-models/ class=md-nav__link> <span class=md-ellipsis> Large Language Models </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/natural-language-processing/ class=md-nav__link> <span class=md-ellipsis> Natural Language Processing </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/neural-machine-translation/ class=md-nav__link> <span class=md-ellipsis> Neural Machine Translation </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/speaker-recognition/ class=md-nav__link> <span class=md-ellipsis> Speaker Recognition </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/speech-classification/ class=md-nav__link> <span class=md-ellipsis> Speech Classification </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/speech-translation/ class=md-nav__link> <span class=md-ellipsis> Speech Translation </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/text-to-speech/ class=md-nav__link> <span class=md-ellipsis> Text to Speech </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/tools/ class=md-nav__link> <span class=md-ellipsis> Tools </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#whats-so-great-about-transducer-models class=md-nav__link> <span class=md-ellipsis> What’s so great about Transducer models? </span> </a> </li> <li class=md-nav__item> <a href=#why-do-transducer-models-consume-a-lot-of-memory class=md-nav__link> <span class=md-ellipsis> Why do Transducer models consume a lot of memory? </span> </a> </li> <li class=md-nav__item> <a href=#enter-numba-support-for-fp16-datatype class=md-nav__link> <span class=md-ellipsis> Enter Numba support for FP16 datatype </span> </a> <nav class=md-nav aria-label="Enter Numba support for FP16 datatype"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#prerequisites class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=#enabling-numba-fp16-support-in-nemo class=md-nav__link> <span class=md-ellipsis> Enabling Numba FP16 Support in NeMo </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#train-a-transducer-asr-model-with-fp16 class=md-nav__link> <span class=md-ellipsis> Train a Transducer ASR model with FP16 </span> </a> </li> <li class=md-nav__item> <a href=#measuring-memory-and-compute-improvements class=md-nav__link> <span class=md-ellipsis> Measuring Memory and Compute Improvements </span> </a> <nav class=md-nav aria-label="Measuring Memory and Compute Improvements"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#results class=md-nav__link> <span class=md-ellipsis> Results </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#conclusion class=md-nav__link> <span class=md-ellipsis> Conclusion </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <!-- Edit button --> <a href=https://github.com/NVIDIA/NeMo/edit/master/docs/blogs/posts/2023/2023-10-28-numba-fp16.md title=edit.link.title class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg> </a> <!-- Back to index button --> <!-- Note: using "align-items:center" to make sure that the arrow looks vertically centered relative to the text --> <a href=../../ class=" md-nav__link" style="align-items:center; color: var(--md-default-fg-color--light); padding-bottom:20px; font-weight: 700;"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> <span class=md-ellipsis> Back to index </span> </a> <!-- Blogging Markup --> <!--        <p class="p-blog">--> <!--        <img src="https://avatars.githubusercontent.com/" alt="@">--> <!--        </p>--> <!--        <p class="p-blog">--> <!-- If number of authors of less than 3, put on new lines --> <aside class=mdx-author> <!-- Add author name --> <span class=post-author> <b>Somshubra Majumdar</b> <!-- Add author GitHub link if 1:1 map exists between author names and author gh ids --> · <a href=https://github.com/titu1994>@titu1994</a> <!-- Add comma if not last author --> </span> <!-- If number of authors is less than 3, put on new lines --> </aside> <!--        <p class="p-blog">--> <!--        <img src="https://avatars.githubusercontent.com/" alt="@">--> <!--        </p>--> <!--        <p class="p-blog">--> <!-- If number of authors of less than 3, put on new lines --> <aside class=mdx-author> <!-- Add author name --> <span class=post-author> <b>Graham Markall</b> <!-- Add author GitHub link if 1:1 map exists between author names and author gh ids --> · <a href=https://github.com/gmarkall>@gmarkall</a> <!-- Add comma if not last author --> </span> <!-- If number of authors is less than 3, put on new lines --> </aside> <p class=p-blog> <span> <!-- Note: using "margin-bottom:-2px" to make sure icons look more-or-less vertically centered relative to the text --> <svg style=margin-bottom:-2px; xmlns=http://www.w3.org/2000/svg width=16 height=16 fill=currentColor class="bi bi-calendar2" viewbox="0 0 16 16"> <path d="M3.5 0a.5.5 0 0 1 .5.5V1h8V.5a.5.5 0 0 1 1 0V1h1a2 2 0 0 1 2 2v11a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V3a2 2 0 0 1 2-2h1V.5a.5.5 0 0 1 .5-.5zM2 2a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h12a1 1 0 0 0 1-1V3a1 1 0 0 0-1-1H2z"/> <path d="M2.5 4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5H3a.5.5 0 0 1-.5-.5V4z"/> </svg> 2023-09-28 · <svg style=margin-bottom:-2px; xmlns=http://www.w3.org/2000/svg width=16 height=16 fill=currentColor class="bi bi-clock" viewbox="0 0 16 16"> <path d="M8 3.5a.5.5 0 0 0-1 0V9a.5.5 0 0 0 .252.434l3.5 2a.5.5 0 0 0 .496-.868L8 8.71V3.5z"/> <path d="M8 16A8 8 0 1 0 8 0a8 8 0 0 0 0 16zm7-8A7 7 0 1 1 1 8a7 7 0 0 1 14 0z"/> </svg> 5 minute read </span> </p> <!-- Markdown content --> <h1 id=training-nemo-rnn-t-models-efficiently-with-numba-fp16-support>Training NeMo RNN-T Models Efficiently with Numba FP16 Support<a class=headerlink href=#training-nemo-rnn-t-models-efficiently-with-numba-fp16-support title="Permanent link">&para;</a></h1> <p>In the field of Automatic Speech Recognition research, <a href=https://arxiv.org/abs/1211.3711>RNN Transducer (RNN-T)</a> is a type of sequence-to-sequence model that is well-known for being able to achieve state-of-the-art transcription accuracy in offline and real-time (A.K.A. "streaming") speech recognition applications. They are also notorious for having high memory requirements. In this blog post we will explain why they have this reputation, and how NeMo allows you to side-step many of the memory requirements issues, including how to make use of Numba’s recent addition of FP16 support.</p> <!-- more --> <h2 id=whats-so-great-about-transducer-models>What’s so great about Transducer models?<a class=headerlink href=#whats-so-great-about-transducer-models title="Permanent link">&para;</a></h2> <p>As we mentioned, RNN-T models (often called just “Transducer” models, since they don’t need to use an RNN) have been shown to achieve state-of-the-art results for accurate, streaming speech recognition. RNN-Ts are also able to handle longer sequences than they were trained on, as well as out-of-vocabulary words, which is a common problem in speech recognition.</p> <p>If you want to learn more about Transducer models, we recommend the excellent blog post <a href=https://lorenlugosch.github.io/posts/2020/11/transducer/ >Sequence-to-sequence learning with Transducers</a>.</p> <figure> <p><img alt="RNN-Transducer architecture" src=https://github.com/NVIDIA/NeMo/releases/download/v1.20.0/asset-post-2023-10-28-numba-fp16-rnnt_joint.png> </p> <figcaption><b>Figure 1.</b> <i>The RNN-Transducer architecture. The audio sequence 'x' is passed through the encoder network, and the text sequence 'y' is passed to the prediction network. The outputs of both networks are combined in the joint network.</i></figcaption> </figure> <h2 id=why-do-transducer-models-consume-a-lot-of-memory>Why do Transducer models consume a lot of memory?<a class=headerlink href=#why-do-transducer-models-consume-a-lot-of-memory title="Permanent link">&para;</a></h2> <p>A significant drawback of the Transducer architecture is the vast GPU memory required during training. As discussed in <a href=https://lorenlugosch.github.io/posts/2020/11/transducer/ >Sequence-to-sequence learning with Transducers</a>, the output of the joint network (which is the final step before the softmax, see Figure 1) in the transducer is a 4-dimensional tensor which occupies significant amounts of memory. The size of this tensor (both its activations and its gradients) can be calculated as follows:</p> <p>\[\textnormal{Joint tensor size} = \: B \times T \times U \times V \times 2 \times 4 \,\, \textnormal{bytes}\] </p> <p>Here, <span class=arithmatex>\(B\)</span> is the batch size, <span class=arithmatex>\(T\)</span> is the audio sequence length, <span class=arithmatex>\(U\)</span> is the text sequence length and <span class=arithmatex>\(V\)</span> is the vocabulary size. We multiply by 2 so we get the size of both the activations and the gradients. We then multiply by 4 because we assume an FP32 datatype (and a single FP32 value occupies 4 bytes).</p> <p>The audio waveform signal is commonly converted to 100 Hz spectrogram frames, which means each second of audio corresponds to 100 audio frames. Thus, for a single 20-second audio clip with about 100 subwords in its transcript, and a vocabulary of 1024 subword tokens, the size of the tensor would be ~1.6 Gigabytes:</p> <p>\[\textnormal{Joint tensor size} = \: B \times \phantom{....}T\phantom{....}\times \phantom{.}U\phantom{.} \times \phantom{..}V\phantom{.} \times 2 \times 4 \,\, \textnormal{bytes}\phantom{=1.6 \textnormal{ Gigabytes}}\] \[\phantom{\textnormal{Joint tensor size}} = \: 1 \times (20 \times 100) \times 100 \times 1024 \times 2 \times 4 \,\, \textnormal{bytes}=1.6 \textnormal{ Gigabytes}\] </p> <p>This number is for a single audio sample. If we use a larger batch size, e.g. 10, for training, we will quickly run out of memory even on 16 GB GPUs. Also, remember, this is just the size of the joint network tensor: there is additional memory required to keep the model in memory, and to calculate the activation and gradients of the rest of the network!</p> <h2 id=enter-numba-support-for-fp16-datatype><i><b>Enter Numba support for FP16 datatype</b></i><a class=headerlink href=#enter-numba-support-for-fp16-datatype title="Permanent link">&para;</a></h2> <p>As of <a href=https://numba.readthedocs.io/en/stable/release-notes.html#version-0-57-0-1-may-2023>Numba 0.57</a> release, FP16 datatype format is now supported natively. Using this, we can effectively halve the memory requirement of the above joint network tensor and support larger batch sizes with almost no changes to our NeMo workflow!</p> <p>NeMo utilizes <a href=https://numba.readthedocs.io/en/stable/index.html>Numba's</a> <a href=https://numba.readthedocs.io/en/latest/cuda/kernels.html>Just-in-time compile CUDA kernels</a> written in Python in order to efficiently compute the RNN-T loss (which requires manipulation of the joint network tensor). This allows a user to simply have Numba installed on their system, and without explicit compilation of C++ / CUDA code, they can train their RNN-T models easily. Furthermore, since the kernels are written in Python, it allows for simple modifications by researchers to develop advanced features such as <a href=https://arxiv.org/abs/2010.11148>FastEmit</a>, and even other extensions to the Transducer loss, such as <a href=https://arxiv.org/abs/2304.06795>Token-and-Duration Transducers</a>.</p> <h3 id=prerequisites>Prerequisites<a class=headerlink href=#prerequisites title="Permanent link">&para;</a></h3> <ul> <li><a href=https://pytorch.org/ >Pytorch</a> 1.13.1+</li> <li><a href=https://github.com/NVIDIA/NeMo>Nvidia NeMo</a> 1.20.0+</li> <li><a href=https://github.com/numba/numba>Numba</a> 0.57+ (<code>conda install numba=0.57.1 -c conda-forge</code>)</li> <li><a href=https://nvidia.github.io/cuda-python/install.html>CUDA Python</a></li> <li>CUDA 11.8 (installed as part of <code>cudatoolkit</code>)</li> <li>It is preferable to install these libraries in a Conda environment (Python 3.10) for correct dependency resolution.</li> </ul> <p>The following snippet can be used to install the requirements:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>conda<span class=w> </span>create<span class=w> </span>-n<span class=w> </span>nemo<span class=w> </span>-c<span class=w> </span>pytorch<span class=w> </span>-c<span class=w> </span>nvidia<span class=w> </span>-c<span class=w> </span>conda-forge<span class=w> </span><span class=nv>python</span><span class=o>=</span><span class=m>3</span>.10<span class=w> </span><span class=nv>numba</span><span class=o>=</span><span class=m>0</span>.57.1<span class=w> </span><span class=nv>cudatoolkit</span><span class=o>=</span><span class=m>11</span>.8<span class=w> </span>cuda-python<span class=o>=</span><span class=m>11</span>.8<span class=w> </span>pytorch<span class=w> </span>torchvision<span class=w> </span>torchaudio<span class=w> </span>pytorch-cuda<span class=o>=</span><span class=m>11</span>.8<span class=w> </span>cython
<a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>conda<span class=w> </span>activate<span class=w> </span>nemo
<a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>pip<span class=w> </span>install<span class=w> </span>nemo-toolkit<span class=o>[</span>all<span class=o>]</span>&gt;<span class=o>=</span><span class=m>1</span>.20.0
</code></pre></div> <h3 id=enabling-numba-fp16-support-in-nemo>Enabling Numba FP16 Support in NeMo<a class=headerlink href=#enabling-numba-fp16-support-in-nemo title="Permanent link">&para;</a></h3> <ul> <li>Set the Numba environment variable: <code>export NUMBA_CUDA_USE_NVIDIA_BINDING=1</code></li> <li>Set the NeMo environment variable: <code>export STRICT_NUMBA_COMPAT_CHECK=0</code></li> <li>Check if installation is successful by using the following snippet: </li> </ul> <div class=highlight><pre><span></span><code><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=kn>from</span> <span class=nn>nemo.core.utils</span> <span class=kn>import</span> <span class=n>numba_utils</span>
<a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a>
<a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=c1># Should be True</span>
<a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a><span class=nb>print</span><span class=p>(</span><span class=n>numba_utils</span><span class=o>.</span><span class=n>numba_cuda_is_supported</span><span class=p>(</span><span class=n>numba_utils</span><span class=o>.</span><span class=n>__NUMBA_MINIMUM_VERSION_FP16_SUPPORTED__</span><span class=p>))</span>
<a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a>
<a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a><span class=c1># Should also be True</span>
<a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a><span class=nb>print</span><span class=p>(</span><span class=n>numba_utils</span><span class=o>.</span><span class=n>is_numba_cuda_fp16_supported</span><span class=p>())</span>
</code></pre></div> <h2 id=train-a-transducer-asr-model-with-fp16>Train a Transducer ASR model with FP16<a class=headerlink href=#train-a-transducer-asr-model-with-fp16 title="Permanent link">&para;</a></h2> <p>With the above environment flags set, and the latest Numba version installed, NeMo supports training with FP16 loss out of the box. For a tutorial on how to setup and train a Transducer ASR model, please refer to the NeMo <a href=https://colab.research.google.com/github/NVIDIA/NeMo/blob/stable/tutorials/asr/ASR_with_Transducers.ipynb>ASR with Transducers</a> tutorial.</p> <p>The only change necessary to use the FP16 loss is to specify <code>trainer.precision=16</code> in the NeMo model config.</p> <h2 id=measuring-memory-and-compute-improvements>Measuring Memory and Compute Improvements<a class=headerlink href=#measuring-memory-and-compute-improvements title="Permanent link">&para;</a></h2> <p>We devised a simple benchmarking script that measures the memory usage when computing the RNN-T loss (with gradients enabled) for various combinations of inputs which are common during training on the Librispeech speech recognition dataset. The script used <a href=https://gist.github.com/titu1994/e786fbd1efccd81f412bf76df5ff41c7>can be found in this Gist</a>.</p> <p>We assume that we are training a <a href=https://arxiv.org/abs/2005.08100>Conformer</a> or <a href=https://arxiv.org/abs/2305.05084>Fast Conformer</a> Transducer model, which performs 4x or 8x audio signal reduction respectively. For Librispeech, the longest audio file is approximately 17 seconds, which becomes approximately 200 timesteps after 8x reduction. We check memory consumption for both Character tokenization (<span class=arithmatex>\(V\)</span>=28) and Subword Tokenization (<span class=arithmatex>\(V\)</span>=1024). Due to the tokenization, the transcript text may be between 80 to 250 tokens but we take a conservative limit of 100 to 200 tokens. As well as the output tensor, the benchmarking script takes into account the memory consumption of the activations and gradients of the intermediate layer of the joint network, which has a shape of <span class=arithmatex>\(B \times T \times U \times H\)</span>. We set <span class=arithmatex>\(H\)</span>=640 (this is a common value for this parameter in the literature).</p> <h3 id=results><i>Results</i><a class=headerlink href=#results title="Permanent link">&para;</a></h3> <p>We show the results of the benchmarking in the graph below. You can see that using FP16 effectively halves the memory cost of an RNN-T model (compared with FP32), allowing to keep the memory usage relatively low as the values of parameters <span class=arithmatex>\(B\)</span>, <span class=arithmatex>\(T\)</span>, <span class=arithmatex>\(U\)</span> and <span class=arithmatex>\(V\)</span> increase.</p> <figure> <p><img alt="RNN-Transducer memory under fp16 vs fp32" src=https://github.com/NVIDIA/NeMo/releases/download/v1.20.0/asset-post-2023-10-28-numba-fp16-memory_joint.png> </p> <figcaption><b>Figure 1.</b> Plot of GPU Memory usage for a given combination of Batch size (B), Timesteps (T), Text length (U), Vocabulary size (V) and the hidden dimension of the RNN-Transducer Joint. </figcaption> </figure> <p>It is to be noted that NVIDIA NeMo has several other mechanisms to significantly reduce peak memory consumption, such as <a href=https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/configs.html#effect-of-batch-splitting-fused-batch-step>Batch Splitting</a>. When combined with FP16 support in Numba, this allows us to train even larger ASR models with a Transducer loss. </p> <h2 id=conclusion>Conclusion<a class=headerlink href=#conclusion title="Permanent link">&para;</a></h2> <p>Numba FP16 support alleviates one of the crucial issues of RNN-Transducer training: memory usage. This unlocks efficient streaming speech recognition model training for a wider audience of researchers and developers. With a simple installation step, users are empowered to <a href=https://colab.research.google.com/github/NVIDIA/NeMo/blob/stable/tutorials/asr/ASR_with_Transducers.ipynb>train</a> and <a href=https://colab.research.google.com/github/NVIDIA/NeMo/blob/stable/tutorials/asr/ASR_CTC_Language_Finetuning.ipynb>fine-tune</a> their own speech recognition solutions on commonly available GPUs.</p> <p>Users can learn more about Numba and how to leverage it for high-performance computing using Python in their <a href=https://numba.readthedocs.io/en/stable/user/index.html>5-minute guide</a>. Furthermore, NeMo users can read up on how to perform speech recognition with many models and losses in the <a href=https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/intro.html>NeMo ASR documentation</a>.</p> <!-- Last update of source file --> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../2023-08-forced-alignment/ class="md-footer__link md-footer__link--prev" aria-label="Previous: How does forced alignment work?"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> How does forced alignment work? </div> </div> </a> <a href=../../2024/2024-01-parakeet/ class="md-footer__link md-footer__link--next" aria-label="Next: Announcing NVIDIA NeMo Parakeet ASR Models for Pushing the Boundaries of Speech Recognition"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Announcing NVIDIA NeMo Parakeet ASR Models for Pushing the Boundaries of Speech Recognition </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2019 - 2023 NVIDIA </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/NVIDIA/NeMo target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["content.code.annotate", "content.tabs.link", "content.tooltips", "navigation.expand", "navigation.indexes", "navigation.path", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.footer", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.e1c3ead8.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>