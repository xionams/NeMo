<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="State of the Art Speech Recognition for Everyone"><meta name=author content="['Nithin Rao Koluguri', 'Somshubra Majumdar']"><link href=https://nvidia.github.io/NeMo/blogs/2024/2024-01-parakeet/ rel=canonical><link href=../../2023/2023-10-28-numba-fp16/ rel=prev><link href=../2024-01-parakeet-tdt/ rel=next><link rel=icon href=../../../assets/favicon.png><meta name=generator content="mkdocs-1.5.3, mkdocs-material-9.5.6"><title>Announcing NVIDIA NeMo Parakeet ASR Models for Pushing the Boundaries of Speech Recognition - NVIDIA NeMo</title><link rel=stylesheet href=../../../assets/stylesheets/main.50c56a3b.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../assets/stylesheets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><!-- Add scripts that need to run before here --><!-- Add scripts that need to run afterward here --><!-- Add Twitter Card metadata --><meta name=twitter:card content=summary_large_image><!-- Add OpenGraph Title metadata --><meta property=og:title content="NVIDIA NeMo Parakeet"><meta name=twitter:title content="NVIDIA NeMo Parakeet"><!-- Add OpenGraph Type metadata --><meta property=og:type content=website><!-- Add OpenGraph URL metadata --><meta content=https://nvidia.github.io/NeMo/blogs/2024/2024-01-parakeet/ property=og:url><!-- Add OpenGraph Image metadata --><meta property=og:image content=https://nvidia.github.io/NeMo/assets/images/nvidia-logo-vert.png><meta property=twitter:image content=https://nvidia.github.io/NeMo/assets/images/nvidia-logo-vert.png><!-- Add OpenGraph Image Type metadata --><meta property=og:image:type content=image/png><!-- Add OpenGraph Description metadata --><meta property=og:description content="State of the Art Speech Recognition for Everyone"><meta property=twitter:description content="State of the Art Speech Recognition for Everyone"></head> <body dir=ltr data-md-color-scheme=midnight-black data-md-color-primary=indigo data-md-color-accent=light-blue> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#announcing-nvidia-nemo-parakeet-asr-models-for-pushing-the-boundaries-of-speech-recognition class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="NVIDIA NeMo" class="md-header__button md-logo" aria-label="NVIDIA NeMo" data-md-component=logo> <svg id=Layer_1 data-name="Layer 1" xmlns=http://www.w3.org/2000/svg viewbox="0 0 16 16"> <defs> <style>
      .cls-1 {
        fill: #76b900;
      }
    </style> </defs> <g id=NVIDIA_Logo data-name="NVIDIA Logo"> <path id=Eye_Mark class=cls-1 d=M5.9698,5.86543V4.90907c.09285-.00661.18663-.01156.28219-.01456A5.75152,5.75152,0,0,1,10.58372,7.142S8.73032,9.71631,6.74309,9.71631a2.40975,2.40975,0,0,1-.77329-.12364v-2.9c1.01832.123,1.223.57279,1.83539,1.59325l1.36157-1.148A3.60517,3.60517,0,0,0,6.49742,5.83431a4.93745,4.93745,0,0,0-.52762.03112m0-3.15922V4.13474c.09389-.00742.1879-.0134.28219-.0168,3.63754-.12254,6.0073,2.98317,6.0073,2.98317s-2.722,3.31-5.55774,3.31a4.18488,4.18488,0,0,1-.73175-.06444v.883a4.81728,4.81728,0,0,0,.60938.03947c2.639,0,4.54736-1.34759,6.39542-2.94267.30618.24532,1.56062.8421,1.8186,1.1037-1.75722,1.47088-5.852,2.65644-8.17346,2.65644-.22369,0-.43886-.01352-.64994-.03376v1.241H16V2.70621Zm0,6.88646v.754A4.26109,4.26109,0,0,1,2.85159,7.37428,5.27645,5.27645,0,0,1,5.9698,5.86543v.8272l-.0038-.0004a2.34214,2.34214,0,0,0-1.81935.83163A3.25091,3.25091,0,0,0,5.9698,9.59267M1.63473,7.26433A6.045,6.045,0,0,1,5.9698,4.90907V4.13474C2.77053,4.39151,0,7.10111,0,7.10111s1.56908,4.53638,5.9698,4.95171v-.82318C2.74044,10.82334,1.63473,7.26433,1.63473,7.26433Z data-name="Eye Mark"/> </g> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> NVIDIA NeMo </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Announcing NVIDIA NeMo Parakeet ASR Models for Pushing the Boundaries of Speech Recognition </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=midnight-black data-md-color-primary=indigo data-md-color-accent=light-blue aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=light-blue aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg> </label> </form> <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/NVIDIA/NeMo title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> NVIDIA/NeMo </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Blog </a> </li> <li class=md-tabs__item> <a href=../../../publications/ class=md-tabs__link> Publications </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="NVIDIA NeMo" class="md-nav__button md-logo" aria-label="NVIDIA NeMo" data-md-component=logo> <svg id=Layer_1 data-name="Layer 1" xmlns=http://www.w3.org/2000/svg viewbox="0 0 16 16"> <defs> <style>
      .cls-1 {
        fill: #76b900;
      }
    </style> </defs> <g id=NVIDIA_Logo data-name="NVIDIA Logo"> <path id=Eye_Mark class=cls-1 d=M5.9698,5.86543V4.90907c.09285-.00661.18663-.01156.28219-.01456A5.75152,5.75152,0,0,1,10.58372,7.142S8.73032,9.71631,6.74309,9.71631a2.40975,2.40975,0,0,1-.77329-.12364v-2.9c1.01832.123,1.223.57279,1.83539,1.59325l1.36157-1.148A3.60517,3.60517,0,0,0,6.49742,5.83431a4.93745,4.93745,0,0,0-.52762.03112m0-3.15922V4.13474c.09389-.00742.1879-.0134.28219-.0168,3.63754-.12254,6.0073,2.98317,6.0073,2.98317s-2.722,3.31-5.55774,3.31a4.18488,4.18488,0,0,1-.73175-.06444v.883a4.81728,4.81728,0,0,0,.60938.03947c2.639,0,4.54736-1.34759,6.39542-2.94267.30618.24532,1.56062.8421,1.8186,1.1037-1.75722,1.47088-5.852,2.65644-8.17346,2.65644-.22369,0-.43886-.01352-.64994-.03376v1.241H16V2.70621Zm0,6.88646v.754A4.26109,4.26109,0,0,1,2.85159,7.37428,5.27645,5.27645,0,0,1,5.9698,5.86543v.8272l-.0038-.0004a2.34214,2.34214,0,0,0-1.81935.83163A3.25091,3.25091,0,0,0,5.9698,9.59267M1.63473,7.26433A6.045,6.045,0,0,1,5.9698,4.90907V4.13474C2.77053,4.39151,0,7.10111,0,7.10111s1.56908,4.53638,5.9698,4.95171v-.82318C2.74044,10.82334,1.63473,7.26433,1.63473,7.26433Z data-name="Eye Mark"/> </g> </svg> </a> NVIDIA NeMo </label> <div class=md-nav__source> <a href=https://github.com/NVIDIA/NeMo title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> NVIDIA/NeMo </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Blog </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../archive/2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> <li class=md-nav__item> <a href=../../archive/2023/ class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> <li class=md-nav__item> <a href=../../archive/2022/ class=md-nav__link> <span class=md-ellipsis> 2022 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../category/announcements/ class=md-nav__link> <span class=md-ellipsis> Announcements </span> </a> </li> <li class=md-nav__item> <a href=../../category/nvidia-technical-blog/ class=md-nav__link> <span class=md-ellipsis> NVIDIA Technical blog </span> </a> </li> <li class=md-nav__item> <a href=../../category/papers/ class=md-nav__link> <span class=md-ellipsis> Papers </span> </a> </li> <li class=md-nav__item> <a href=../../category/technical-deep-dive/ class=md-nav__link> <span class=md-ellipsis> Technical deep-dive </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../../../publications/ class="md-nav__link "> <span class=md-ellipsis> Publications </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Publications </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../publications/archive/2023/ class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/archive/2022/ class=md-nav__link> <span class=md-ellipsis> 2022 </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/archive/2021/ class=md-nav__link> <span class=md-ellipsis> 2021 </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/archive/2020/ class=md-nav__link> <span class=md-ellipsis> 2020 </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/archive/2019/ class=md-nav__link> <span class=md-ellipsis> 2019 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_3> <label class=md-nav__link for=__nav_3_3 id=__nav_3_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../publications/category/inverse-text-normalization/ class=md-nav__link> <span class=md-ellipsis> (Inverse) Text Normalization </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/automatic-speech-recognition/ class=md-nav__link> <span class=md-ellipsis> Automatic Speech Recognition </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/dialog-state-tracking/ class=md-nav__link> <span class=md-ellipsis> Dialog State Tracking </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/large-language-models/ class=md-nav__link> <span class=md-ellipsis> Large Language Models </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/natural-language-processing/ class=md-nav__link> <span class=md-ellipsis> Natural Language Processing </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/neural-machine-translation/ class=md-nav__link> <span class=md-ellipsis> Neural Machine Translation </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/speaker-recognition/ class=md-nav__link> <span class=md-ellipsis> Speaker Recognition </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/speech-classification/ class=md-nav__link> <span class=md-ellipsis> Speech Classification </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/speech-translation/ class=md-nav__link> <span class=md-ellipsis> Speech Translation </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/text-to-speech/ class=md-nav__link> <span class=md-ellipsis> Text to Speech </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/tools/ class=md-nav__link> <span class=md-ellipsis> Tools </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#architecture-details class=md-nav__link> <span class=md-ellipsis> Architecture Details </span> </a> </li> <li class=md-nav__item> <a href=#usage class=md-nav__link> <span class=md-ellipsis> Usage </span> </a> </li> <li class=md-nav__item> <a href=#long-form-speech-inference class=md-nav__link> <span class=md-ellipsis> Long-Form Speech Inference </span> </a> </li> <li class=md-nav__item> <a href=#additional-resources class=md-nav__link> <span class=md-ellipsis> Additional Resources </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <!-- Edit button --> <a href=https://github.com/NVIDIA/NeMo/edit/master/docs/blogs/posts/2024/2024-01-parakeet.md title=edit.link.title class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg> </a> <!-- Back to index button --> <!-- Note: using "align-items:center" to make sure that the arrow looks vertically centered relative to the text --> <a href=../../ class=" md-nav__link" style="align-items:center; color: var(--md-default-fg-color--light); padding-bottom:20px; font-weight: 700;"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> <span class=md-ellipsis> Back to index </span> </a> <!-- Blogging Markup --> <!--        <p class="p-blog">--> <!--        <img src="https://avatars.githubusercontent.com/" alt="@">--> <!--        </p>--> <!--        <p class="p-blog">--> <!-- If number of authors of less than 3, put on new lines --> <aside class=mdx-author> <!-- Add author name --> <span class=post-author> <b>Nithin Rao Koluguri</b> <!-- Add author GitHub link if 1:1 map exists between author names and author gh ids --> · <a href=https://github.com/nithinraok>@nithinraok</a> <!-- Add comma if not last author --> </span> <!-- If number of authors is less than 3, put on new lines --> </aside> <!--        <p class="p-blog">--> <!--        <img src="https://avatars.githubusercontent.com/" alt="@">--> <!--        </p>--> <!--        <p class="p-blog">--> <!-- If number of authors of less than 3, put on new lines --> <aside class=mdx-author> <!-- Add author name --> <span class=post-author> <b>Somshubra Majumdar</b> <!-- Add author GitHub link if 1:1 map exists between author names and author gh ids --> · <a href=https://github.com/titu1994>@titu1994</a> <!-- Add comma if not last author --> </span> <!-- If number of authors is less than 3, put on new lines --> </aside> <p class=p-blog> <span> <!-- Note: using "margin-bottom:-2px" to make sure icons look more-or-less vertically centered relative to the text --> <svg style=margin-bottom:-2px; xmlns=http://www.w3.org/2000/svg width=16 height=16 fill=currentColor class="bi bi-calendar2" viewbox="0 0 16 16"> <path d="M3.5 0a.5.5 0 0 1 .5.5V1h8V.5a.5.5 0 0 1 1 0V1h1a2 2 0 0 1 2 2v11a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V3a2 2 0 0 1 2-2h1V.5a.5.5 0 0 1 .5-.5zM2 2a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h12a1 1 0 0 0 1-1V3a1 1 0 0 0-1-1H2z"/> <path d="M2.5 4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5H3a.5.5 0 0 1-.5-.5V4z"/> </svg> 2024-01-03 · <svg style=margin-bottom:-2px; xmlns=http://www.w3.org/2000/svg width=16 height=16 fill=currentColor class="bi bi-clock" viewbox="0 0 16 16"> <path d="M8 3.5a.5.5 0 0 0-1 0V9a.5.5 0 0 0 .252.434l3.5 2a.5.5 0 0 0 .496-.868L8 8.71V3.5z"/> <path d="M8 16A8 8 0 1 0 8 0a8 8 0 0 0 0 16zm7-8A7 7 0 1 1 1 8a7 7 0 0 1 14 0z"/> </svg> 5 minute read </span> </p> <!-- Markdown content --> <h1 id=announcing-nvidia-nemo-parakeet-asr-models-for-pushing-the-boundaries-of-speech-recognition>Announcing NVIDIA NeMo Parakeet ASR Models for Pushing the Boundaries of Speech Recognition<a class=headerlink href=#announcing-nvidia-nemo-parakeet-asr-models-for-pushing-the-boundaries-of-speech-recognition title="Permanent link">&para;</a></h1> <p><a href=https://nvidia.github.io/NeMo/ >NVIDIA NeMo</a>, a leading open-source toolkit for conversational AI, announces the release of Parakeet, a family of state-of-the-art automatic speech recognition (ASR) models (Figure 1.), capable of transcribing spoken English with exceptional accuracy. Developed in collaboration with <a href=http://suno.ai/ >Suno.ai</a>, Parakeet ASR models mark a significant leap forward in speech recognition, paving the way for more natural and efficient human-computer interactions.</p> <figure> <p><img alt="HuggingFace Leaderboard" src=https://github.com/NVIDIA/NeMo/releases/download/v1.21.0/asset-post-2024-01-03-parakeet_leaderboard.png> </p> <figcaption><b>Figure 1.</b> <i> HuggingFace Leaderboard as of 01/03/2024. </i></figcaption> </figure> <p>NVIDIA announces four Parakeet models based on RNN Transducer / Connectionist Temporal Classification decoders and the size of the models. They boast 0.6-1.1 billion parameters and capable of tackling diverse audio environments. Trained on only a 64,000-hour dataset encompassing various accents, domains, and noise conditions, the models deliver exceptional word error rate (WER) performance across benchmark datasets, outperforming previous models.</p> <ul> <li><a href=https://huggingface.co/nvidia/parakeet-rnnt-1.1b>Parakeet RNNT 1.1B</a> - Best recognition accuracy, modest inference speed. Best used when the most accurate transcriptions are necessary.</li> <li><a href=https://huggingface.co/nvidia/parakeet-ctc-1.1b>Parakeet CTC 1.1B</a> - Fast inference, strong recognition accuracy. A great middle ground between accuracy and speed of inference.</li> <li><a href=https://huggingface.co/nvidia/parakeet-rnnt-0.6b>Parakeet RNNT 0.6B</a> - Strong recognition accuracy and fast inference. Useful for large-scale inference on limited resources.</li> <li><a href=https://huggingface.co/nvidia/parakeet-ctc-0.6b>Parakeet CTC 0.6B</a> - Fastest speed, modest recognition accuracy. Useful when transcription speed is the most important.</li> </ul> <!-- more --> <p>Parakeet models exhibit resilience against non-speech segments, including music and silence, effectively preventing the generation of hallucinated transcripts.</p> <p>Built using the <a href=https://github.com/NVIDIA/NeMo>NVIDIA NeMo toolkit</a>, Parakeet prioritizes user-friendliness and flexibility. With pre-trained checkpoints readily available, integrating the model into your projects is a breeze. Whether looking for immediate inference capabilities or fine-tuning for specific tasks, NeMo provides a robust and intuitive framework to leverage the model's full potential.</p> <p>Key benefits of Parakeet models:</p> <ul> <li><strong>State-of-the-art accuracy:</strong> Superior WER performance across diverse audio sources and domains with strong robustness to non-speech segments.</li> <li><strong>Different model sizes:</strong> Two models with 0.6B and 1.1B parameters for robust comprehension of complex speech patterns.</li> <li><strong>Open-source and extensibility:</strong> Built on NVIDIA NeMo, allowing for seamless integration and customization.</li> <li><strong>Pre-trained checkpoints:</strong> Ready-to-use models for inference or fine-tuning.</li> <li><strong>Permissive license:</strong> Released under CC-BY-4.0 license, model checkpoints can be used in any commercial application.</li> </ul> <p>Parakeet is a major step forward in the evolution of conversational AI. Its exceptional accuracy, coupled with the flexibility and ease of use offered by NeMo, empowers developers to create more natural and intuitive voice-powered applications. The possibilities are endless, from enhancing the accuracy of virtual assistants to enabling seamless real-time communication.</p> <p>The Parakeet family of models achieves state-of-the-art numbers on the <a href=https://huggingface.co/spaces/hf-audio/open_asr_leaderboard>HuggingFace Leaderboard</a>. Users can try out the <a href=https://huggingface.co/nvidia/parakeet-rnnt-1.1b>parakeet-rnnt-1.1b</a> firsthand at the <a href=https://huggingface.co/spaces/nvidia/parakeet-rnnt-1.1b>Gradio demo</a>. To access the model locally and explore the toolkit, visit the <a href=https://github.com/NVIDIA/NeMo>NVIDIA NeMo Github page</a>.</p> <h2 id=architecture-details>Architecture Details<a class=headerlink href=#architecture-details title="Permanent link">&para;</a></h2> <p>Parakeet models are based on the <a href=https://arxiv.org/abs/2305.05084>Fast Conformer architecture published in ASRU 2023</a>. Fast Conformer is an optimized version of the Conformer model with 8x depthwise-separable convolutional downsampling, modified convolution kernel size, and an efficient subsampling module. Additionally it supports inference on very long audio segments (up to 11 hours of speech) on an A100 80GB card using Local Attention. The model is trained end-to-end using the Transducer decoder (RNNT) or Connectionist Temporal Classification decoder. For further details on long audio inference, please refer to the ICASSP 2024 paper “<a href=https://arxiv.org/abs/2309.09950>Investigating End-to-End ASR Architectures for Long Form Audio Transcription</a>”.</p> <figure> <p><img alt="Parakeet architecture" src=https://github.com/NVIDIA/NeMo/releases/download/v1.21.0/asset-post-2024-01-03-parakeet_arch.png> </p> <figcaption><b>Figure 2.</b> <i> Fast Conformer Architecture shows blocks of downsampling, conformer encoder blocks with limited context attention (LCA), and global token (GT).</i></figcaption> </figure> <h2 id=usage>Usage<a class=headerlink href=#usage title="Permanent link">&para;</a></h2> <p>NVIDIA NeMo can be installed as a pip package as shown below. Cython and PyTorch (2.0 and above) should be installed before attempting to install NeMo Toolkit.</p> <p>Then simply use: <div class=highlight><pre><span></span><code><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>pip<span class=w> </span>install<span class=w> </span>nemo_toolkit<span class=o>[</span><span class=s1>&#39;asr&#39;</span><span class=o>]</span>
</code></pre></div></p> <p>Once installed, you can evaluate a list of audio files as follows: <div class=highlight><pre><span></span><code><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=kn>import</span> <span class=nn>nemo.collections.asr</span> <span class=k>as</span> <span class=nn>nemo_asr</span>
<a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=n>asr_model</span> <span class=o>=</span> <span class=n>nemo_asr</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>ASRModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=o>=</span><span class=s2>&quot;nvidia/parakeet-rnnt-1.1b&quot;</span><span class=p>)</span>
<a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=n>transcript</span> <span class=o>=</span> <span class=n>asr_model</span><span class=o>.</span><span class=n>transcribe</span><span class=p>([</span><span class=s2>&quot;some_audio_file.wav&quot;</span><span class=p>])</span>
</code></pre></div></p> <h2 id=long-form-speech-inference>Long-Form Speech Inference<a class=headerlink href=#long-form-speech-inference title="Permanent link">&para;</a></h2> <p>Once you have a Fast Conformer model loaded, you can easily modify the attention type to limited context attention after building the model. You can also apply audio chunking for the subsampling module to perform inference on huge audio files!</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>These models were trained with global attention, and switching to local attention will degrade their performance. However, they will still be able to transcribe long audio files reasonably well.</p> </div> <p>For limited context attention on huge files (upto 11 hours on an A100), perform the following steps:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=c1># Enable local attention</span>
<a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=n>asr_model</span><span class=o>.</span><span class=n>change_attention_model</span><span class=p>(</span><span class=s2>&quot;rel_pos_local_attn&quot;</span><span class=p>,</span> <span class=p>[</span><span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>])</span>  <span class=c1># local attn</span>
<a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a>
<a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a><span class=c1># Enable chunking for subsampling module</span>
<a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=n>asr_model</span><span class=o>.</span><span class=n>change_subsampling_conv_chunking_factor</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># 1 = auto select</span>
<a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a>
<a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a><span class=c1># Transcribe a huge audio file</span>
<a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a><span class=n>asr_model</span><span class=o>.</span><span class=n>transcribe</span><span class=p>([</span><span class=s2>&quot;&lt;path to a huge audio file&gt;.wav&quot;</span><span class=p>])</span>  <span class=c1># 10+ hours !</span>
</code></pre></div> <h2 id=additional-resources>Additional Resources<a class=headerlink href=#additional-resources title="Permanent link">&para;</a></h2> <ul> <li><a href=https://huggingface.co/spaces/hf-audio/open_asr_leaderboard>HuggingFace ASR Leaderboard</a></li> <li><a href=https://github.com/huggingface/open_asr_leaderboard>HuggingFace ASR Leaderboard Evaluation</a></li> <li><a href="https://huggingface.co/models?library=nemo&sort=trending&search=parakee">NeMo Parakeet Models on HuggingFace</a></li> <li><a href=https://github.com/NVIDIA/NeMo>NVIDIA NeMo Webpage</a></li> <li><a href=https://docs.nvidia.com/deeplearning/nemo/user-guide/index.html>NVIDIA NeMo ASR Documentation</a></li> <li>Papers:<ul> <li><a href=https://arxiv.org/abs/2305.05084>Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition</a></li> <li><a href=https://arxiv.org/abs/2309.09950>Investigating End-to-End ASR Architectures for Long Form Audio Transcription</a></li> </ul> </li> </ul> <!-- Last update of source file --> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../2023/2023-10-28-numba-fp16/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Training NeMo RNN-T Models Efficiently with Numba FP16 Support"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Training NeMo RNN-T Models Efficiently with Numba FP16 Support </div> </div> </a> <a href=../2024-01-parakeet-tdt/ class="md-footer__link md-footer__link--next" aria-label="Next: Unveiling NVIDIA NeMo's Parakeet-TDT -- Turbocharged ASR with Unrivaled Accuracy"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Unveiling NVIDIA NeMo's Parakeet-TDT -- Turbocharged ASR with Unrivaled Accuracy </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2019 - 2023 NVIDIA </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/NVIDIA/NeMo target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["content.code.annotate", "content.tabs.link", "content.tooltips", "navigation.expand", "navigation.indexes", "navigation.path", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.footer", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.e1c3ead8.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>