<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="NeMo: a toolkit for conversational AI & NLP"><link href=https://nvidia.github.io/NeMo/publications/category/automatic-speech-recognition/page/2/ rel=canonical><link href=../../../inverse-text-normalization/ rel=prev><link href=../../../dialog-state-tracking/ rel=next><link rel=icon href=../../../../../assets/favicon.png><meta name=generator content="mkdocs-1.5.3, mkdocs-material-9.5.6"><title>Automatic Speech Recognition - NVIDIA NeMo</title><link rel=stylesheet href=../../../../../assets/stylesheets/main.50c56a3b.min.css><link rel=stylesheet href=../../../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../../../assets/stylesheets/extra.css><script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><!-- Add scripts that need to run before here --><!-- Add scripts that need to run afterward here --><!-- Add Twitter Card metadata --><meta name=twitter:card content=summary_large_image><!-- Add OpenGraph Title metadata --><meta property=og:title content="NVIDIA NeMo"><meta name=twitter:title content="NVIDIA NeMo"><!-- Add OpenGraph Type metadata --><meta property=og:type content=website><!-- Add OpenGraph URL metadata --><meta content=https://nvidia.github.io/NeMo/publications/category/automatic-speech-recognition/page/2/ property=og:url><!-- Add OpenGraph Image metadata --><meta property=og:image content=https://nvidia.github.io/NeMo/assets/images/nvidia-logo-vert.png><meta property=twitter:image content=https://nvidia.github.io/NeMo/assets/images/nvidia-logo-vert.png><!-- Add OpenGraph Image Type metadata --><meta property=og:image:type content=image/png><!-- Add OpenGraph Description metadata --><meta property=og:description content="NeMo: a toolkit for conversational AI & NLP"><meta property=twitter:description content="NeMo: a toolkit for conversational AI & NLP"></head> <body dir=ltr data-md-color-scheme=midnight-black data-md-color-primary=indigo data-md-color-accent=light-blue> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#automatic-speech-recognition class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../../../.. title="NVIDIA NeMo" class="md-header__button md-logo" aria-label="NVIDIA NeMo" data-md-component=logo> <svg id=Layer_1 data-name="Layer 1" xmlns=http://www.w3.org/2000/svg viewbox="0 0 16 16"> <defs> <style>
      .cls-1 {
        fill: #76b900;
      }
    </style> </defs> <g id=NVIDIA_Logo data-name="NVIDIA Logo"> <path id=Eye_Mark class=cls-1 d=M5.9698,5.86543V4.90907c.09285-.00661.18663-.01156.28219-.01456A5.75152,5.75152,0,0,1,10.58372,7.142S8.73032,9.71631,6.74309,9.71631a2.40975,2.40975,0,0,1-.77329-.12364v-2.9c1.01832.123,1.223.57279,1.83539,1.59325l1.36157-1.148A3.60517,3.60517,0,0,0,6.49742,5.83431a4.93745,4.93745,0,0,0-.52762.03112m0-3.15922V4.13474c.09389-.00742.1879-.0134.28219-.0168,3.63754-.12254,6.0073,2.98317,6.0073,2.98317s-2.722,3.31-5.55774,3.31a4.18488,4.18488,0,0,1-.73175-.06444v.883a4.81728,4.81728,0,0,0,.60938.03947c2.639,0,4.54736-1.34759,6.39542-2.94267.30618.24532,1.56062.8421,1.8186,1.1037-1.75722,1.47088-5.852,2.65644-8.17346,2.65644-.22369,0-.43886-.01352-.64994-.03376v1.241H16V2.70621Zm0,6.88646v.754A4.26109,4.26109,0,0,1,2.85159,7.37428,5.27645,5.27645,0,0,1,5.9698,5.86543v.8272l-.0038-.0004a2.34214,2.34214,0,0,0-1.81935.83163A3.25091,3.25091,0,0,0,5.9698,9.59267M1.63473,7.26433A6.045,6.045,0,0,1,5.9698,4.90907V4.13474C2.77053,4.39151,0,7.10111,0,7.10111s1.56908,4.53638,5.9698,4.95171v-.82318C2.74044,10.82334,1.63473,7.26433,1.63473,7.26433Z data-name="Eye Mark"/> </g> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> NVIDIA NeMo </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Automatic Speech Recognition </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=midnight-black data-md-color-primary=indigo data-md-color-accent=light-blue aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=light-blue aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg> </label> </form> <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/NVIDIA/NeMo title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> NVIDIA/NeMo </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../../../blogs/ class=md-tabs__link> Blog </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../../../ class=md-tabs__link> Publications </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../../.. title="NVIDIA NeMo" class="md-nav__button md-logo" aria-label="NVIDIA NeMo" data-md-component=logo> <svg id=Layer_1 data-name="Layer 1" xmlns=http://www.w3.org/2000/svg viewbox="0 0 16 16"> <defs> <style>
      .cls-1 {
        fill: #76b900;
      }
    </style> </defs> <g id=NVIDIA_Logo data-name="NVIDIA Logo"> <path id=Eye_Mark class=cls-1 d=M5.9698,5.86543V4.90907c.09285-.00661.18663-.01156.28219-.01456A5.75152,5.75152,0,0,1,10.58372,7.142S8.73032,9.71631,6.74309,9.71631a2.40975,2.40975,0,0,1-.77329-.12364v-2.9c1.01832.123,1.223.57279,1.83539,1.59325l1.36157-1.148A3.60517,3.60517,0,0,0,6.49742,5.83431a4.93745,4.93745,0,0,0-.52762.03112m0-3.15922V4.13474c.09389-.00742.1879-.0134.28219-.0168,3.63754-.12254,6.0073,2.98317,6.0073,2.98317s-2.722,3.31-5.55774,3.31a4.18488,4.18488,0,0,1-.73175-.06444v.883a4.81728,4.81728,0,0,0,.60938.03947c2.639,0,4.54736-1.34759,6.39542-2.94267.30618.24532,1.56062.8421,1.8186,1.1037-1.75722,1.47088-5.852,2.65644-8.17346,2.65644-.22369,0-.43886-.01352-.64994-.03376v1.241H16V2.70621Zm0,6.88646v.754A4.26109,4.26109,0,0,1,2.85159,7.37428,5.27645,5.27645,0,0,1,5.9698,5.86543v.8272l-.0038-.0004a2.34214,2.34214,0,0,0-1.81935.83163A3.25091,3.25091,0,0,0,5.9698,9.59267M1.63473,7.26433A6.045,6.045,0,0,1,5.9698,4.90907V4.13474C2.77053,4.39151,0,7.10111,0,7.10111s1.56908,4.53638,5.9698,4.95171v-.82318C2.74044,10.82334,1.63473,7.26433,1.63473,7.26433Z data-name="Eye Mark"/> </g> </svg> </a> NVIDIA NeMo </label> <div class=md-nav__source> <a href=https://github.com/NVIDIA/NeMo title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> NVIDIA/NeMo </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=../../../../../blogs/ class="md-nav__link "> <span class=md-ellipsis> Blog </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../../blogs/archive/2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> <li class=md-nav__item> <a href=../../../../../blogs/archive/2023/ class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> <li class=md-nav__item> <a href=../../../../../blogs/archive/2022/ class=md-nav__link> <span class=md-ellipsis> 2022 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../../blogs/category/announcements/ class=md-nav__link> <span class=md-ellipsis> Announcements </span> </a> </li> <li class=md-nav__item> <a href=../../../../../blogs/category/nvidia-technical-blog/ class=md-nav__link> <span class=md-ellipsis> NVIDIA Technical blog </span> </a> </li> <li class=md-nav__item> <a href=../../../../../blogs/category/papers/ class=md-nav__link> <span class=md-ellipsis> Papers </span> </a> </li> <li class=md-nav__item> <a href=../../../../../blogs/category/technical-deep-dive/ class=md-nav__link> <span class=md-ellipsis> Technical deep-dive </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <div class="md-nav__link md-nav__container"> <a href=../../../../ class="md-nav__link "> <span class=md-ellipsis> Publications </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Publications </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../archive/2023/ class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2022/ class=md-nav__link> <span class=md-ellipsis> 2022 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2021/ class=md-nav__link> <span class=md-ellipsis> 2021 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2020/ class=md-nav__link> <span class=md-ellipsis> 2020 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2019/ class=md-nav__link> <span class=md-ellipsis> 2019 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_3 checked> <label class=md-nav__link for=__nav_3_3 id=__nav_3_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../inverse-text-normalization/ class=md-nav__link> <span class=md-ellipsis> (Inverse) Text Normalization </span> </a> </li> <li class=md-nav__item> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Automatic Speech Recognition </span> <span class="md-nav__icon md-icon"></span> </label> <a href=../../ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Automatic Speech Recognition </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#carnelinet-neural-mixture-model-for-automatic-speech-recognition class=md-nav__link> <span class=md-ellipsis> CarneliNet: Neural Mixture Model for Automatic Speech Recognition </span> </a> </li> <li class=md-nav__item> <a href=#cross-language-transfer-learning-and-domain-adaptation-for-end-to-end-automatic-speech-recognition class=md-nav__link> <span class=md-ellipsis> Cross-Language Transfer Learning and Domain Adaptation for End-to-End Automatic Speech Recognition </span> </a> </li> <li class=md-nav__item> <a href=#citrinet-closing-the-gap-between-non-autoregressive-and-autoregressive-end-to-end-models-for-automatic-speech-recognition class=md-nav__link> <span class=md-ellipsis> Citrinet: Closing the Gap between Non-Autoregressive and Autoregressive End-to-End Models for Automatic Speech Recognition </span> </a> </li> <li class=md-nav__item> <a href=#improving-noise-robustness-of-an-end-to-end-neural-model-for-automatic-speech-recognition class=md-nav__link> <span class=md-ellipsis> Improving Noise Robustness of an End-to-End Neural Model for Automatic Speech Recognition </span> </a> </li> <li class=md-nav__item> <a href=#correction-of-automatic-speech-recognition-with-transformer-sequence-to-sequence-model class=md-nav__link> <span class=md-ellipsis> Correction of Automatic Speech Recognition with Transformer Sequence-To-Sequence Model </span> </a> </li> <li class=md-nav__item> <a href=#quartznet-deep-automatic-speech-recognition-with-1d-time-channel-separable-convolutions class=md-nav__link> <span class=md-ellipsis> QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions </span> </a> </li> <li class=md-nav__item> <a href=#jasper-an-end-to-end-convolutional-neural-acoustic-model class=md-nav__link> <span class=md-ellipsis> Jasper: An End-to-End Convolutional Neural Acoustic Model </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../dialog-state-tracking/ class=md-nav__link> <span class=md-ellipsis> Dialog State Tracking </span> </a> </li> <li class=md-nav__item> <a href=../../../large-language-models/ class=md-nav__link> <span class=md-ellipsis> Large Language Models </span> </a> </li> <li class=md-nav__item> <a href=../../../natural-language-processing/ class=md-nav__link> <span class=md-ellipsis> Natural Language Processing </span> </a> </li> <li class=md-nav__item> <a href=../../../neural-machine-translation/ class=md-nav__link> <span class=md-ellipsis> Neural Machine Translation </span> </a> </li> <li class=md-nav__item> <a href=../../../speaker-recognition/ class=md-nav__link> <span class=md-ellipsis> Speaker Recognition </span> </a> </li> <li class=md-nav__item> <a href=../../../speech-classification/ class=md-nav__link> <span class=md-ellipsis> Speech Classification </span> </a> </li> <li class=md-nav__item> <a href=../../../speech-translation/ class=md-nav__link> <span class=md-ellipsis> Speech Translation </span> </a> </li> <li class=md-nav__item> <a href=../../../text-to-speech/ class=md-nav__link> <span class=md-ellipsis> Text to Speech </span> </a> </li> <li class=md-nav__item> <a href=../../../tools/ class=md-nav__link> <span class=md-ellipsis> Tools </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#carnelinet-neural-mixture-model-for-automatic-speech-recognition class=md-nav__link> <span class=md-ellipsis> CarneliNet: Neural Mixture Model for Automatic Speech Recognition </span> </a> </li> <li class=md-nav__item> <a href=#cross-language-transfer-learning-and-domain-adaptation-for-end-to-end-automatic-speech-recognition class=md-nav__link> <span class=md-ellipsis> Cross-Language Transfer Learning and Domain Adaptation for End-to-End Automatic Speech Recognition </span> </a> </li> <li class=md-nav__item> <a href=#citrinet-closing-the-gap-between-non-autoregressive-and-autoregressive-end-to-end-models-for-automatic-speech-recognition class=md-nav__link> <span class=md-ellipsis> Citrinet: Closing the Gap between Non-Autoregressive and Autoregressive End-to-End Models for Automatic Speech Recognition </span> </a> </li> <li class=md-nav__item> <a href=#improving-noise-robustness-of-an-end-to-end-neural-model-for-automatic-speech-recognition class=md-nav__link> <span class=md-ellipsis> Improving Noise Robustness of an End-to-End Neural Model for Automatic Speech Recognition </span> </a> </li> <li class=md-nav__item> <a href=#correction-of-automatic-speech-recognition-with-transformer-sequence-to-sequence-model class=md-nav__link> <span class=md-ellipsis> Correction of Automatic Speech Recognition with Transformer Sequence-To-Sequence Model </span> </a> </li> <li class=md-nav__item> <a href=#quartznet-deep-automatic-speech-recognition-with-1d-time-channel-separable-convolutions class=md-nav__link> <span class=md-ellipsis> QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions </span> </a> </li> <li class=md-nav__item> <a href=#jasper-an-end-to-end-convolutional-neural-acoustic-model class=md-nav__link> <span class=md-ellipsis> Jasper: An End-to-End Convolutional Neural Acoustic Model </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <div class=md-content__inner> <header class=md-typeset> <h1 id=automatic-speech-recognition>Automatic Speech Recognition<a class=headerlink href=#automatic-speech-recognition title="Permanent link">&para;</a></h1> </header> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Aleksei Kalinov</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Somshubra Majumdar</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Jagadeesh Balam</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Boris Ginsburg</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2021-07-22 00:00:00">July 22, 2021</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../ class=md-meta__link>Automatic Speech Recognition</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=carnelinet-neural-mixture-model-for-automatic-speech-recognition><a href=../../../../2021/2021-carnelinet/ class=toclink><a href=https://arxiv.org/abs/2107.10708>CarneliNet: Neural Mixture Model for Automatic Speech Recognition</a></a></h2> <p>End-to-end automatic speech recognition systems have achieved great accuracy by using deeper and deeper models. However, the increased depth comes with a larger receptive field that can negatively impact model performance in streaming scenarios. We propose an alternative approach that we call Neural Mixture Model. The basic idea is to introduce a parallel mixture of shallow networks instead of a very deep network. To validate this idea we design CarneliNet -- a CTC-based neural network composed of three mega-blocks. Each mega-block consists of multiple parallel shallow sub-networks based on 1D depthwise-separable convolutions. We evaluate the model on LibriSpeech, MLS and AISHELL-2 datasets and achieved close to state-of-the-art results for CTC-based models. Finally, we demonstrate that one can dynamically reconfigure the number of parallel sub-networks to accommodate the computational requirements without retraining.</p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://arxiv.org/abs/2107.10708 class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Jian Luo</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Jianzong Wang</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Ning Cheng</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Edward Xiao</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Jing Xiao</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Georg Kucsko</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Patrick O’Neill</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Jagadeesh Balam</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Slyne Deng</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Adriana Flores</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Boris Ginsburg</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Jocelyn Huang</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Oleksii Kuchaiev</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Vitaly Lavrukhin</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Jason Li</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2021-06-09 00:00:00">June 9, 2021</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../ class=md-meta__link>Automatic Speech Recognition</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=cross-language-transfer-learning-and-domain-adaptation-for-end-to-end-automatic-speech-recognition><a href=../../../../2021/2021-cross-language-domain-transfer/ class=toclink><a href=https://ieeexplore.ieee.org/document/9428334>Cross-Language Transfer Learning and Domain Adaptation for End-to-End Automatic Speech Recognition</a></a></h2> <p>In this paper, we demonstrate the efficacy of transfer learning and continuous learning for various automatic speech recognition (ASR) tasks using end-to-end models trained with CTC loss. We start with a large pre-trained English ASR model and show that transfer learning can be effectively and easily performed on: (1) different English accents, (2) different languages (from English to German, Spanish, Russian, or from Mandarin to Cantonese) and (3) application-specific domains. Our extensive set of experiments demonstrate that in all three cases, transfer learning from a good base model has higher accuracy than a model trained from scratch. Our results indicate that, for fine-tuning, larger pre-trained models are better than small pre-trained models, even if the dataset for fine-tuning is small. We also show that transfer learning significantly speeds up convergence, which could result in significant cost savings when training with large datasets.</p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://ieeexplore.ieee.org/document/9428334 class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Somshubra Majumdar</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Jagadeesh Balam</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Oleksii Hrinchuk</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Vitaly Lavrukhin</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Vahid Noroozi</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Boris Ginsburg</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2021-03-21 00:00:00">March 21, 2021</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../ class=md-meta__link>Automatic Speech Recognition</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=citrinet-closing-the-gap-between-non-autoregressive-and-autoregressive-end-to-end-models-for-automatic-speech-recognition><a href=../../../../2021/2021-citrinet/ class=toclink><a href=https://arxiv.org/abs/2104.01721>Citrinet: Closing the Gap between Non-Autoregressive and Autoregressive End-to-End Models for Automatic Speech Recognition</a></a></h2> <p>We propose Citrinet - a new end-to-end convolutional Connectionist Temporal Classification (CTC) based automatic speech recognition (ASR) model. Citrinet is deep residual neural model which uses 1D time-channel separable convolutions combined with sub-word encoding and squeeze-and-excitation. The resulting architecture significantly reduces the gap between non-autoregressive and sequence-to-sequence and transducer models. We evaluate Citrinet on LibriSpeech, TED-LIUM2, AISHELL-1 and Multilingual LibriSpeech (MLS) English speech datasets. Citrinet accuracy on these datasets is close to the best autoregressive Transducer models.</p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://arxiv.org/abs/2104.01721 class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Jagadeesh Balam</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Jocelyn Huang</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Vitaly Lavrukhin</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Slyne Deng</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Somshubra Majumdar</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Boris Ginsburg</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2020-10-23 00:00:00">October 23, 2020</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../ class=md-meta__link>Automatic Speech Recognition</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=improving-noise-robustness-of-an-end-to-end-neural-model-for-automatic-speech-recognition><a href=../../../../2020/2020-asr-noise-robustness/ class=toclink><a href=https://arxiv.org/abs/2010.12715>Improving Noise Robustness of an End-to-End Neural Model for Automatic Speech Recognition</a></a></h2> <p>We present our experiments in training robust to noise an end-to-end automatic speech recognition (ASR) model using intensive data augmentation. We explore the efficacy of fine-tuning a pre-trained model to improve noise robustness, and we find it to be a very efficient way to train for various noisy conditions, especially when the conditions in which the model will be used, are unknown. Starting with a model trained on clean data helps establish baseline performance on clean speech. We carefully fine-tune this model to both maintain the performance on clean speech, and improve the model accuracy in noisy conditions. With this schema, we trained robust to noise English and Mandarin ASR models on large public corpora. All described models and training recipes are open sourced in NeMo, a toolkit for conversational AI.</p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://arxiv.org/abs/2010.12715 class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Oleksii Hrinchuk</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Mariya Popova</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Boris Ginsburg</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2020-04-20 00:00:00">April 20, 2020</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../ class=md-meta__link>Automatic Speech Recognition</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=correction-of-automatic-speech-recognition-with-transformer-sequence-to-sequence-model><a href=../../../../2020/2020-asr-correction-with-transformers/ class=toclink><a href=https://ieeexplore.ieee.org/abstract/document/9053051>Correction of Automatic Speech Recognition with Transformer Sequence-To-Sequence Model</a></a></h2> <p>In this work, we introduce a simple yet efficient post-processing model for automatic speech recognition. Our model has Transformer-based encoder-decoder architecture which "translates" acoustic model output into grammatically and semantically correct text. We investigate different strategies for regularizing and optimizing the model and show that extensive data augmentation and the initialization with pretrained weights are required to achieve good performance. On the LibriSpeech benchmark, our method demonstrates significant improvement in word error rate over the baseline acoustic model with greedy decoding, especially on much noisier dev-other and test-other portions of the evaluation dataset. Our model also outperforms baseline with 6-gram language model re-scoring and approaches the performance of re-scoring with Transformer-XL neural language model.</p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://ieeexplore.ieee.org/abstract/document/9053051 class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Samuel Kriman</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Stanislav Beliaev</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Boris Ginsburg</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Jocelyn Huang</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Oleksii Kuchaiev</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Vitaly Lavrukhin</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Ryan Leary</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Jason Li</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Yang Zhang</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2019-10-22 00:00:00">October 22, 2019</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../ class=md-meta__link>Automatic Speech Recognition</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=quartznet-deep-automatic-speech-recognition-with-1d-time-channel-separable-convolutions><a href=../../../../2019/2019-quartznet/ class=toclink><a href=https://arxiv.org/abs/1910.10261>QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions</a></a></h2> <p>We propose a new end-to-end neural acoustic model for automatic speech recognition. The model is composed of multiple blocks with residual connections between them. Each block consists of one or more modules with 1D time-channel separable convolutional layers, batch normalization, and ReLU layers. It is trained with CTC loss. The proposed network achieves near state-of-the-art accuracy on LibriSpeech and Wall Street Journal, while having fewer parameters than all competing models. We also demonstrate that this model can be effectively fine-tuned on new datasets.</p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://arxiv.org/abs/1910.10261 class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Jason Li</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Vitaly Lavrukhin</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Boris Ginsburg</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Ryan Leary</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Oleksii Kuchaiev</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Jonathan M. Cohen</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Huyen Nguyen</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Ravi Teja Gadde</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2019-08-05 00:00:00">August 5, 2019</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../ class=md-meta__link>Automatic Speech Recognition</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=jasper-an-end-to-end-convolutional-neural-acoustic-model><a href=../../../../2019/2019-jasper/ class=toclink><a href=https://arxiv.org/abs/1904.03288>Jasper: An End-to-End Convolutional Neural Acoustic Model</a></a></h2> <p>In this paper, we report state-of-the-art results on LibriSpeech among end-to-end speech recognition models without any external training data. Our model, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout, and residual connections. To improve training, we further introduce a new layer-wise optimizer called NovoGrad. Through experiments, we demonstrate that the proposed deep architecture performs as well or better than more complex choices. Our deepest Jasper variant uses 54 convolutional layers. With this architecture, we achieve 2.95% WER using a beam-search decoder with an external neural language model and 3.86% WER with a greedy decoder on LibriSpeech test-clean. We also report competitive results on the Wall Street Journal and the Hub5'00 conversational evaluation datasets. </p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://arxiv.org/abs/1904.03288 class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <nav class=md-pagination> <a href=../../ class=md-pagination__link>1</a> <span class=md-pagination__current>2</span> </nav> </div> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../../inverse-text-normalization/ class="md-footer__link md-footer__link--prev" aria-label="Previous: (Inverse) Text Normalization"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> (Inverse) Text Normalization </div> </div> </a> <a href=../../../dialog-state-tracking/ class="md-footer__link md-footer__link--next" aria-label="Next: Dialog State Tracking"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Dialog State Tracking </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2019 - 2023 NVIDIA </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/NVIDIA/NeMo target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../../../..", "features": ["content.code.annotate", "content.tabs.link", "content.tooltips", "navigation.expand", "navigation.indexes", "navigation.path", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.footer", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../../../assets/javascripts/bundle.e1c3ead8.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>