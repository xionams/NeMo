<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="NeMo: a toolkit for conversational AI & NLP"><link href=https://nvidia.github.io/NeMo/publications/archive/2022/ rel=canonical><link href=../2023/ rel=prev><link href=../2021/ rel=next><link rel=icon href=../../../assets/favicon.png><meta name=generator content="mkdocs-1.5.3, mkdocs-material-9.5.6"><title>2022 - NVIDIA NeMo</title><link rel=stylesheet href=../../../assets/stylesheets/main.50c56a3b.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../assets/stylesheets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><!-- Add scripts that need to run before here --><!-- Add scripts that need to run afterward here --><!-- Add Twitter Card metadata --><meta name=twitter:card content=summary_large_image><!-- Add OpenGraph Title metadata --><meta property=og:title content="NVIDIA NeMo"><meta name=twitter:title content="NVIDIA NeMo"><!-- Add OpenGraph Type metadata --><meta property=og:type content=website><!-- Add OpenGraph URL metadata --><meta content=https://nvidia.github.io/NeMo/publications/archive/2022/ property=og:url><!-- Add OpenGraph Image metadata --><meta property=og:image content=https://nvidia.github.io/NeMo/assets/images/nvidia-logo-vert.png><meta property=twitter:image content=https://nvidia.github.io/NeMo/assets/images/nvidia-logo-vert.png><!-- Add OpenGraph Image Type metadata --><meta property=og:image:type content=image/png><!-- Add OpenGraph Description metadata --><meta property=og:description content="NeMo: a toolkit for conversational AI & NLP"><meta property=twitter:description content="NeMo: a toolkit for conversational AI & NLP"></head> <body dir=ltr data-md-color-scheme=midnight-black data-md-color-primary=indigo data-md-color-accent=light-blue> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#2022 class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="NVIDIA NeMo" class="md-header__button md-logo" aria-label="NVIDIA NeMo" data-md-component=logo> <svg id=Layer_1 data-name="Layer 1" xmlns=http://www.w3.org/2000/svg viewbox="0 0 16 16"> <defs> <style>
      .cls-1 {
        fill: #76b900;
      }
    </style> </defs> <g id=NVIDIA_Logo data-name="NVIDIA Logo"> <path id=Eye_Mark class=cls-1 d=M5.9698,5.86543V4.90907c.09285-.00661.18663-.01156.28219-.01456A5.75152,5.75152,0,0,1,10.58372,7.142S8.73032,9.71631,6.74309,9.71631a2.40975,2.40975,0,0,1-.77329-.12364v-2.9c1.01832.123,1.223.57279,1.83539,1.59325l1.36157-1.148A3.60517,3.60517,0,0,0,6.49742,5.83431a4.93745,4.93745,0,0,0-.52762.03112m0-3.15922V4.13474c.09389-.00742.1879-.0134.28219-.0168,3.63754-.12254,6.0073,2.98317,6.0073,2.98317s-2.722,3.31-5.55774,3.31a4.18488,4.18488,0,0,1-.73175-.06444v.883a4.81728,4.81728,0,0,0,.60938.03947c2.639,0,4.54736-1.34759,6.39542-2.94267.30618.24532,1.56062.8421,1.8186,1.1037-1.75722,1.47088-5.852,2.65644-8.17346,2.65644-.22369,0-.43886-.01352-.64994-.03376v1.241H16V2.70621Zm0,6.88646v.754A4.26109,4.26109,0,0,1,2.85159,7.37428,5.27645,5.27645,0,0,1,5.9698,5.86543v.8272l-.0038-.0004a2.34214,2.34214,0,0,0-1.81935.83163A3.25091,3.25091,0,0,0,5.9698,9.59267M1.63473,7.26433A6.045,6.045,0,0,1,5.9698,4.90907V4.13474C2.77053,4.39151,0,7.10111,0,7.10111s1.56908,4.53638,5.9698,4.95171v-.82318C2.74044,10.82334,1.63473,7.26433,1.63473,7.26433Z data-name="Eye Mark"/> </g> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> NVIDIA NeMo </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 2022 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=midnight-black data-md-color-primary=indigo data-md-color-accent=light-blue aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=light-blue aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg> </label> </form> <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/NVIDIA/NeMo title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> NVIDIA/NeMo </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../blogs/ class=md-tabs__link> Blog </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Publications </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="NVIDIA NeMo" class="md-nav__button md-logo" aria-label="NVIDIA NeMo" data-md-component=logo> <svg id=Layer_1 data-name="Layer 1" xmlns=http://www.w3.org/2000/svg viewbox="0 0 16 16"> <defs> <style>
      .cls-1 {
        fill: #76b900;
      }
    </style> </defs> <g id=NVIDIA_Logo data-name="NVIDIA Logo"> <path id=Eye_Mark class=cls-1 d=M5.9698,5.86543V4.90907c.09285-.00661.18663-.01156.28219-.01456A5.75152,5.75152,0,0,1,10.58372,7.142S8.73032,9.71631,6.74309,9.71631a2.40975,2.40975,0,0,1-.77329-.12364v-2.9c1.01832.123,1.223.57279,1.83539,1.59325l1.36157-1.148A3.60517,3.60517,0,0,0,6.49742,5.83431a4.93745,4.93745,0,0,0-.52762.03112m0-3.15922V4.13474c.09389-.00742.1879-.0134.28219-.0168,3.63754-.12254,6.0073,2.98317,6.0073,2.98317s-2.722,3.31-5.55774,3.31a4.18488,4.18488,0,0,1-.73175-.06444v.883a4.81728,4.81728,0,0,0,.60938.03947c2.639,0,4.54736-1.34759,6.39542-2.94267.30618.24532,1.56062.8421,1.8186,1.1037-1.75722,1.47088-5.852,2.65644-8.17346,2.65644-.22369,0-.43886-.01352-.64994-.03376v1.241H16V2.70621Zm0,6.88646v.754A4.26109,4.26109,0,0,1,2.85159,7.37428,5.27645,5.27645,0,0,1,5.9698,5.86543v.8272l-.0038-.0004a2.34214,2.34214,0,0,0-1.81935.83163A3.25091,3.25091,0,0,0,5.9698,9.59267M1.63473,7.26433A6.045,6.045,0,0,1,5.9698,4.90907V4.13474C2.77053,4.39151,0,7.10111,0,7.10111s1.56908,4.53638,5.9698,4.95171v-.82318C2.74044,10.82334,1.63473,7.26433,1.63473,7.26433Z data-name="Eye Mark"/> </g> </svg> </a> NVIDIA NeMo </label> <div class=md-nav__source> <a href=https://github.com/NVIDIA/NeMo title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> NVIDIA/NeMo </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=../../../blogs/ class="md-nav__link "> <span class=md-ellipsis> Blog </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../blogs/archive/2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> <li class=md-nav__item> <a href=../../../blogs/archive/2023/ class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> <li class=md-nav__item> <a href=../../../blogs/archive/2022/ class=md-nav__link> <span class=md-ellipsis> 2022 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../blogs/category/announcements/ class=md-nav__link> <span class=md-ellipsis> Announcements </span> </a> </li> <li class=md-nav__item> <a href=../../../blogs/category/nvidia-technical-blog/ class=md-nav__link> <span class=md-ellipsis> NVIDIA Technical blog </span> </a> </li> <li class=md-nav__item> <a href=../../../blogs/category/papers/ class=md-nav__link> <span class=md-ellipsis> Papers </span> </a> </li> <li class=md-nav__item> <a href=../../../blogs/category/technical-deep-dive/ class=md-nav__link> <span class=md-ellipsis> Technical deep-dive </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Publications </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Publications </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_2 checked> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=true> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../2023/ class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 2022 </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 2022 </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#accidental-learners-spoken-language-identification-in-multilingual-self-supervised-models class=md-nav__link> <span class=md-ellipsis> Accidental Learners: Spoken Language Identification in Multilingual Self-Supervised Models </span> </a> </li> <li class=md-nav__item> <a href=#multi-blank-transducers-for-speech-recognition class=md-nav__link> <span class=md-ellipsis> Multi-blank Transducers for Speech Recognition </span> </a> </li> <li class=md-nav__item> <a href=#adapter-based-extension-of-multi-speaker-text-to-speech-model-for-new-speakers class=md-nav__link> <span class=md-ellipsis> Adapter-Based Extension of Multi-Speaker Text-to-Speech Model for New Speakers </span> </a> </li> <li class=md-nav__item> <a href=#a-compact-end-to-end-model-with-local-and-global-context-for-spoken-language-identification class=md-nav__link> <span class=md-ellipsis> A Compact End-to-End Model with Local and Global Context for Spoken Language Identification </span> </a> </li> <li class=md-nav__item> <a href=#evaluating-parameter-efficient-learning-for-generation class=md-nav__link> <span class=md-ellipsis> Evaluating Parameter Efficient Learning for Generation </span> </a> </li> <li class=md-nav__item> <a href=#thutmose-tagger-single-pass-neural-model-for-inverse-text-normalization class=md-nav__link> <span class=md-ellipsis> Thutmose Tagger: Single-pass neural model for Inverse Text Normalization </span> </a> </li> <li class=md-nav__item> <a href=#finding-the-right-recipe-for-low-resource-domain-adaptation-in-neural-machine-translation class=md-nav__link> <span class=md-ellipsis> Finding the Right Recipe for Low Resource Domain Adaptation in Neural Machine Translation </span> </a> </li> <li class=md-nav__item> <a href=#nvidia-nemo-offline-speech-translation-systems-for-iwslt-2022 class=md-nav__link> <span class=md-ellipsis> NVIDIA NeMo Offline Speech Translation Systems for IWSLT 2022 </span> </a> </li> <li class=md-nav__item> <a href=#titanet-neural-model-for-speaker-representation-with-1d-depth-wise-separable-convolutions-and-global-context class=md-nav__link> <span class=md-ellipsis> TitaNet: Neural Model for Speaker Representation with 1D Depth-Wise Separable Convolutions and Global Context </span> </a> </li> <li class=md-nav__item> <a href=#shallow-fusion-of-weighted-finite-state-transducer-and-language-model-for-text-normalization class=md-nav__link> <span class=md-ellipsis> Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../2021/ class=md-nav__link> <span class=md-ellipsis> 2021 </span> </a> </li> <li class=md-nav__item> <a href=../2020/ class=md-nav__link> <span class=md-ellipsis> 2020 </span> </a> </li> <li class=md-nav__item> <a href=../2019/ class=md-nav__link> <span class=md-ellipsis> 2019 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_3> <label class=md-nav__link for=__nav_3_3 id=__nav_3_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../category/inverse-text-normalization/ class=md-nav__link> <span class=md-ellipsis> (Inverse) Text Normalization </span> </a> </li> <li class=md-nav__item> <a href=../../category/automatic-speech-recognition/ class=md-nav__link> <span class=md-ellipsis> Automatic Speech Recognition </span> </a> </li> <li class=md-nav__item> <a href=../../category/dialog-state-tracking/ class=md-nav__link> <span class=md-ellipsis> Dialog State Tracking </span> </a> </li> <li class=md-nav__item> <a href=../../category/large-language-models/ class=md-nav__link> <span class=md-ellipsis> Large Language Models </span> </a> </li> <li class=md-nav__item> <a href=../../category/natural-language-processing/ class=md-nav__link> <span class=md-ellipsis> Natural Language Processing </span> </a> </li> <li class=md-nav__item> <a href=../../category/neural-machine-translation/ class=md-nav__link> <span class=md-ellipsis> Neural Machine Translation </span> </a> </li> <li class=md-nav__item> <a href=../../category/speaker-recognition/ class=md-nav__link> <span class=md-ellipsis> Speaker Recognition </span> </a> </li> <li class=md-nav__item> <a href=../../category/speech-classification/ class=md-nav__link> <span class=md-ellipsis> Speech Classification </span> </a> </li> <li class=md-nav__item> <a href=../../category/speech-translation/ class=md-nav__link> <span class=md-ellipsis> Speech Translation </span> </a> </li> <li class=md-nav__item> <a href=../../category/text-to-speech/ class=md-nav__link> <span class=md-ellipsis> Text to Speech </span> </a> </li> <li class=md-nav__item> <a href=../../category/tools/ class=md-nav__link> <span class=md-ellipsis> Tools </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#accidental-learners-spoken-language-identification-in-multilingual-self-supervised-models class=md-nav__link> <span class=md-ellipsis> Accidental Learners: Spoken Language Identification in Multilingual Self-Supervised Models </span> </a> </li> <li class=md-nav__item> <a href=#multi-blank-transducers-for-speech-recognition class=md-nav__link> <span class=md-ellipsis> Multi-blank Transducers for Speech Recognition </span> </a> </li> <li class=md-nav__item> <a href=#adapter-based-extension-of-multi-speaker-text-to-speech-model-for-new-speakers class=md-nav__link> <span class=md-ellipsis> Adapter-Based Extension of Multi-Speaker Text-to-Speech Model for New Speakers </span> </a> </li> <li class=md-nav__item> <a href=#a-compact-end-to-end-model-with-local-and-global-context-for-spoken-language-identification class=md-nav__link> <span class=md-ellipsis> A Compact End-to-End Model with Local and Global Context for Spoken Language Identification </span> </a> </li> <li class=md-nav__item> <a href=#evaluating-parameter-efficient-learning-for-generation class=md-nav__link> <span class=md-ellipsis> Evaluating Parameter Efficient Learning for Generation </span> </a> </li> <li class=md-nav__item> <a href=#thutmose-tagger-single-pass-neural-model-for-inverse-text-normalization class=md-nav__link> <span class=md-ellipsis> Thutmose Tagger: Single-pass neural model for Inverse Text Normalization </span> </a> </li> <li class=md-nav__item> <a href=#finding-the-right-recipe-for-low-resource-domain-adaptation-in-neural-machine-translation class=md-nav__link> <span class=md-ellipsis> Finding the Right Recipe for Low Resource Domain Adaptation in Neural Machine Translation </span> </a> </li> <li class=md-nav__item> <a href=#nvidia-nemo-offline-speech-translation-systems-for-iwslt-2022 class=md-nav__link> <span class=md-ellipsis> NVIDIA NeMo Offline Speech Translation Systems for IWSLT 2022 </span> </a> </li> <li class=md-nav__item> <a href=#titanet-neural-model-for-speaker-representation-with-1d-depth-wise-separable-convolutions-and-global-context class=md-nav__link> <span class=md-ellipsis> TitaNet: Neural Model for Speaker Representation with 1D Depth-Wise Separable Convolutions and Global Context </span> </a> </li> <li class=md-nav__item> <a href=#shallow-fusion-of-weighted-finite-state-transducer-and-language-model-for-text-normalization class=md-nav__link> <span class=md-ellipsis> Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <div class=md-content__inner> <header class=md-typeset> <h1 id=2022>2022<a class=headerlink href=#2022 title="Permanent link">&para;</a></h1> </header> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Travis M. Bartley</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Fei Jia</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Krishna C. Puvvada</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Samuel Kriman</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Boris Ginsburg</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2022-11-09 00:00:00">November 9, 2022</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../category/speech-classification/ class=md-meta__link>Speech Classification</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=accidental-learners-spoken-language-identification-in-multilingual-self-supervised-models><a href=../../2022/2022-accidental-learners-slid/ class=toclink><a href=https://arxiv.org/abs/2211.05103>Accidental Learners: Spoken Language Identification in Multilingual Self-Supervised Models</a></a></h2> <p>In this paper, we extend previous self-supervised approaches for language identification by experimenting with Conformer based architecture in a multilingual pre-training paradigm. We find that pre-trained speech models optimally encode language discriminatory information in lower layers. Further, we demonstrate that the embeddings obtained from these layers are significantly robust to classify unseen languages and different acoustic environments without additional training. After fine-tuning a pre-trained Conformer model on the VoxLingua107 dataset, we achieve results similar to current state-of-the-art systems for language identification. More, our model accomplishes this with 5x less parameters. We open-source the model through the NVIDIA NeMo toolkit.</p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://arxiv.org/abs/2210.15781 class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Hainan Xu</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Fei Jia</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Somshubra Majumdar</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Shinji Watanabe</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Boris Ginsburg</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2022-11-04 00:00:00">November 4, 2022</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../category/automatic-speech-recognition/ class=md-meta__link>Automatic Speech Recognition</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=multi-blank-transducers-for-speech-recognition><a href=../../2022/2022-multiblank-rnnt/ class=toclink><a href=https://arxiv.org/abs/2211.03541>Multi-blank Transducers for Speech Recognition</a></a></h2> <p>This paper proposes a modification to RNN-Transducer (RNN-T) models for automatic speech recognition (ASR). In standard RNN-T, the emission of a blank symbol consumes exactly one input frame; in our proposed method, we introduce additional blank symbols, which consume two or more input frames when emitted. We refer to the added symbols as big blanks, and the method multi-blank RNN-T. For training multi-blank RNN-Ts, we propose a novel logit under-normalization method in order to prioritize emissions of big blanks. With experiments on multiple languages and datasets, we show that multi-blank RNN-T methods could bring relative speedups of over +90%/+139% to model inference for English Librispeech and German Multilingual Librispeech datasets, respectively. The multi-blank RNN-T method also improves ASR accuracy consistently. We will release our implementation of the method in the NeMo (\url{this https URL}) toolkit.</p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://arxiv.org/abs/2211.03541 class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Cheng-Ping Hsieh</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Subhankar Ghosh</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Boris Ginsburg</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2022-11-01 00:00:00">November 1, 2022</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../category/text-to-speech/ class=md-meta__link>Text to Speech</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=adapter-based-extension-of-multi-speaker-text-to-speech-model-for-new-speakers><a href=../../2022/2022-multi-speaker-tts-adapter/ class=toclink><a href=https://arxiv.org/abs/2211.00585>Adapter-Based Extension of Multi-Speaker Text-to-Speech Model for New Speakers</a></a></h2> <p>Fine-tuning is a popular method for adapting text-to-speech (TTS) models to new speakers. However this approach has some challenges. Usually fine-tuning requires several hours of high quality speech per speaker. There is also that fine-tuning will negatively affect the quality of speech synthesis for previously learnt speakers. In this paper we propose an alternative approach for TTS adaptation based on using parameter-efficient adapter modules. In the proposed approach, a few small adapter modules are added to the original network. The original weights are frozen, and only the adapters are fine-tuned on speech for new speaker. The parameter-efficient fine-tuning approach will produce a new model with high level of parameter sharing with original model. Our experiments on LibriTTS, HiFi-TTS and VCTK datasets validate the effectiveness of adapter-based method through objective and subjective metrics.</p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://arxiv.org/abs/2211.00585 class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Fei Jia</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Nithin Rao Koluguri</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Jagadeesh Balam</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Boris Ginsburg</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2022-10-27 00:00:00">October 27, 2022</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../category/speech-classification/ class=md-meta__link>Speech Classification</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=a-compact-end-to-end-model-with-local-and-global-context-for-spoken-language-identification><a href=../../2022/2022-ambernet/ class=toclink><a href=https://arxiv.org/abs/2210.15781>A Compact End-to-End Model with Local and Global Context for Spoken Language Identification</a></a></h2> <p>We introduce TitaNet-LID, a compact end-to-end neural network for Spoken Language Identification (LID) that is based on the ContextNet architecture. TitaNet-LID employs 1D depth-wise separable convolutions and Squeeze-and-Excitation layers to effectively capture local and global context within an utterance. Despite its small size, TitaNet-LID achieves performance similar to state-of-the-art models on the VoxLingua107 dataset while being 10 times smaller. Furthermore, it can be easily adapted to new acoustic conditions and unseen languages through simple fine-tuning, achieving a state-of-the-art accuracy of 88.2% on the FLEURS benchmark. Our model is scalable and can achieve a better trade-off between accuracy and speed. TitaNet-LID performs well even on short utterances less than 5s in length, indicating its robustness to input length.</p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://arxiv.org/abs/2210.15781 class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Peng Xu</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Mostofa Patwary</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Shrimai Prabhumoye</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Virginia Adams</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Ryan J. Prenger</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Wei Ping</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Nayeon Lee</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Mohammad Shoeybi</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Bryan Catanzaro</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2022-10-25 00:00:00">October 25, 2022</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../category/natural-language-processing/ class=md-meta__link>Natural Language Processing</a>, <a href=../../category/large-language-models/ class=md-meta__link>Large Language Models</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=evaluating-parameter-efficient-learning-for-generation><a href=../../2022/2022-efficient-peft/ class=toclink><a href=https://arxiv.org/abs/2210.13673>Evaluating Parameter Efficient Learning for Generation</a></a></h2> <p>Parameter efficient learning methods (PERMs) have recently gained significant attention as they provide an efficient way for pre-trained language models (PLMs) to adapt to a downstream task. However, these conclusions are mostly drawn from in-domain evaluations over the full training set. In this paper, we present comparisons between PERMs and finetuning from three new perspectives: (1) the effect of sample and model size to in-domain evaluations, (2) generalization to unseen domains and new datasets, and (3) the faithfulness of generations. Our results show that for in-domain settings (a) there is a cross point of sample size for which PERMs will perform better than finetuning when training with fewer samples, and (b) larger PLMs have larger cross points. For cross-domain and cross-dataset cases, we show that (a) Adapter (Houlsby et al., 2019) performs the best amongst all the PERMs studied here, and (b) it outperforms finetuning if the task dataset is below a certain size. We also compare the faithfulness of generations and show that PERMs can achieve better faithfulness score than finetuning, especially for small training set, by as much as 6%. Finally, we apply Adapter to MT-NLG 530b (Smith et al., 2022) and achieve new state-of-the-art results on Xsum (Narayan et al., 2018) for all ROUGE scores (ROUGE-1 49.17, ROUGE-2 27.20, ROUGE-L 40.98).</p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://arxiv.org/abs/2210.13673 class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Alexandra Antonova</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Evelina Bakhturina</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Boris Ginsburg</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2022-08-30 00:00:00">August 30, 2022</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../category/inverse-text-normalization/ class=md-meta__link>(Inverse) Text Normalization</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=thutmose-tagger-single-pass-neural-model-for-inverse-text-normalization><a href=../../2022/2022-thutmose-tagger/ class=toclink><a href=https://arxiv.org/abs/2208.00064>Thutmose Tagger: Single-pass neural model for Inverse Text Normalization</a></a></h2> <p>Inverse text normalization (ITN) is an essential post-processing step in automatic speech recognition (ASR). It converts numbers, dates, abbreviations, and other semiotic classes from the spoken form generated by ASR to their written forms. One can consider ITN as a Machine Translation task and use neural sequence-to-sequence models to solve it. Unfortunately, such neural models are prone to hallucinations that could lead to unacceptable errors. To mitigate this issue, we propose a single-pass token classifier model that regards ITN as a tagging task. The model assigns a replacement fragment to every input token or marks it for deletion or copying without changes. We present a dataset preparation method based on the granular alignment of ITN examples. The proposed model is less prone to hallucination errors. The model is trained on the Google Text Normalization dataset and achieves state-of-the-art sentence accuracy on both English and Russian test sets. One-to-one correspondence between tags and input words improves the interpretability of the model's predictions, simplifies debugging, and allows for post-processing corrections. The model is simpler than sequence-to-sequence models and easier to optimize in production settings. The model and the code to prepare the dataset is published as part of NeMo project.</p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://arxiv.org/abs/2208.00064 class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Virginia Adams</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Sandeep Subramanian</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Mike Chrzanowski</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Oleksii Hrinchuk</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Oleksii Kuchaiev</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2022-06-02 00:00:00">June 2, 2022</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../category/neural-machine-translation/ class=md-meta__link>Neural Machine Translation</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=finding-the-right-recipe-for-low-resource-domain-adaptation-in-neural-machine-translation><a href=../../2022/2022-low-resource-nmt-adaptation/ class=toclink><a href=https://arxiv.org/abs/2206.01137>Finding the Right Recipe for Low Resource Domain Adaptation in Neural Machine Translation</a></a></h2> <p>General translation models often still struggle to generate accurate translations in specialized domains. To guide machine translation practitioners and characterize the effectiveness of domain adaptation methods under different data availability scenarios, we conduct an in-depth empirical exploration of monolingual and parallel data approaches to domain adaptation of pre-trained, third-party, NMT models in settings where architecture change is impractical. We compare data centric adaptation methods in isolation and combination. We study method effectiveness in very low resource (8k parallel examples) and moderately low resource (46k parallel examples) conditions and propose an ensemble approach to alleviate reductions in original domain translation quality. Our work includes three domains: consumer electronic, clinical, and biomedical and spans four language pairs - Zh-En, Ja-En, Es-En, and Ru-En. We also make concrete recommendations for achieving high in-domain performance and release our consumer electronic and medical domain datasets for all languages and make our code publicly available.</p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://arxiv.org/abs/2206.01137 class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Oleksii Hrinchuk</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Vahid Noroozi</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Abhinav Khattar</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Anton Peganov</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Sandeep Subramanian</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Somshubra Majumdar</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Oleksii Kuchaiev</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2022-05-01 00:00:00">May 1, 2022</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../category/speech-translation/ class=md-meta__link>Speech Translation</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=nvidia-nemo-offline-speech-translation-systems-for-iwslt-2022><a href=../../2022/2022-speech-translation/ class=toclink><a href=https://aclanthology.org/2022.iwslt-1.18/ >NVIDIA NeMo Offline Speech Translation Systems for IWSLT 2022</a></a></h2> <p>This paper provides an overview of NVIDIA NeMo’s speech translation systems for the IWSLT 2022 Offline Speech Translation Task. Our cascade system consists of 1) Conformer RNN-T automatic speech recognition model, 2) punctuation-capitalization model based on pre-trained T5 encoder, 3) ensemble of Transformer neural machine translation models fine-tuned on TED talks. Our end-to-end model has less parameters and consists of Conformer encoder and Transformer decoder. It relies on the cascade system by re-using its pre-trained ASR encoder and training on synthetic translations generated with the ensemble of NMT models. Our En-&gt;De cascade and end-to-end systems achieve 29.7 and 26.2 BLEU on the 2020 test set correspondingly, both outperforming the previous year’s best of 26 BLEU.</p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://aclanthology.org/2022.iwslt-1.18/ class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Nithin Rao Koluguri</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Taejin Park</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Boris Ginsburg</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2022-04-22 00:00:00">April 22, 2022</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../category/speaker-recognition/ class=md-meta__link>Speaker Recognition</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=titanet-neural-model-for-speaker-representation-with-1d-depth-wise-separable-convolutions-and-global-context><a href=../../2022/2022-titanet/ class=toclink><a href=https://ieeexplore.ieee.org/abstract/document/9746806>TitaNet: Neural Model for Speaker Representation with 1D Depth-Wise Separable Convolutions and Global Context</a></a></h2> <p>In this paper, we propose TitaNet, a novel neural network architecture for extracting speaker representations. We employ 1D depth-wise separable convolutions with Squeeze-and-Excitation (SE) layers with global context followed by channel attention based statistics pooling layer to map variable-length utterances to a fixed-length embedding (t-vector). TitaNet is a scalable architecture and achieves state-of-the-art performance on speaker verification task with an equal error rate (EER) of 0.68% on the VoxCeleb1 trial file and also on speaker diarization tasks with diarization error rate (DER) of 1.73% on AMI-MixHeadset, 1.99% on AMI-Lapel and 1.11% on CH109. Furthermore, we investigate various sizes of TitaNet and present a light TitaNet-S model with only 6M parameters that achieve near state-of-the-art results in diarization tasks.</p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://ieeexplore.ieee.org/abstract/document/9746806 class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <!-- Docs: https://github.com/squidfunk/mkdocs-material/blob/27a1e7c3eea7d9ca212f1e82cae37d5424303bce/src/templates/partials/post.html#L92 --> <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Post excerpt --> <article class="md-post md-post--excerpt"> <!-- Specifically for cases where we have the continue_url override, we add the author list to the preview --> <header class=md-post__header> <nav class> <span class=post-author__small> <b>Evelina Bakhturina</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Yang Zhang</b> <!-- Add comma if not last author --> , </span> <span class=post-author__small> <b>Boris Ginsburg</b> <!-- Add comma if not last author --> </span> </nav> </header> <header class=md-post__header> <!-- Post authors --> <!--    --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <!-- Post date --> <li class=md-meta__item> <time datetime="2022-03-29 00:00:00">March 29, 2022</time></li> <!-- Post categories --> <li class=md-meta__item> in <a href=../../category/inverse-text-normalization/ class=md-meta__link>(Inverse) Text Normalization</a></li> <!-- Post readtime --> <li class=md-meta__item> 30 min read </li> </ul> <!-- Draft marker --> </div> </header> <!-- Post content --> <div class="md-post__content md-typeset"> <h2 id=shallow-fusion-of-weighted-finite-state-transducer-and-language-model-for-text-normalization><a href=../../2022/2022-fusion-wfst-norm/ class=toclink><a href=https://arxiv.org/abs/2203.15917>Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization</a></a></h2> <p>Text normalization (TN) systems in production are largely rule-based using weighted finite-state transducers (WFST). However, WFST-based systems struggle with ambiguous input when the normalized form is context-dependent. On the other hand, neural text normalization systems can take context into account but they suffer from unrecoverable errors and require labeled normalization datasets, which are hard to collect. We propose a new hybrid approach that combines the benefits of rule-based and neural systems. First, a non-deterministic WFST outputs all normalization candidates, and then a neural language model picks the best one -- similar to shallow fusion for automatic speech recognition. While the WFST prevents unrecoverable errors, the language model resolves contextual ambiguity. The approach is easy to extend and we show it is effective. It achieves comparable or better results than existing state-of-the-art TN models.</p> <!-- !!! MODIFICATION !!! : Wrap <a> in the .md-button .md-button--primary css classes --> <!-- Continue reading link --> <nav class=md-post__action> <!-- First check for `post.meta.continue_url`; if present, use that redirect url; otherwise point to post --> <a href=https://arxiv.org/abs/2203.15917 class="md-button with-right-arrow"> Continue reading </a> </nav> <!-- !!! MODIFICATION !!! : Add Horizontal Bar --> <hr> </div> </article> <nav class=md-pagination> </nav> </div> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../2023/ class="md-footer__link md-footer__link--prev" aria-label="Previous: 2023"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> 2023 </div> </div> </a> <a href=../2021/ class="md-footer__link md-footer__link--next" aria-label="Next: 2021"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> 2021 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2019 - 2023 NVIDIA </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/NVIDIA/NeMo target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["content.code.annotate", "content.tabs.link", "content.tooltips", "navigation.expand", "navigation.indexes", "navigation.path", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.footer", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.e1c3ead8.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>